{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoc3ZgH0vzsd"
   },
   "source": [
    "# High Level Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code and data associated with the Cell Matter Publication on the multi-agent system dubbed MULTITASK.
    A. Gilad Kusne, NIST, aaron.kusne@nist.gov, Release 3/1/2023
    If using this work for a publication, please cite:
    Kusne, A. Gilad, et al. "Scalable Multi-Agent Lab Framework for Lab Optimization" Matter 2023.

    Packages used:
    torch==1.11.0
    tensorflow==2.8.2
    tabulate==0.8.9
    simpy==4.0.1
    scipy==1.6.2
    scikit-learn==0.24.1
    pandas==1.2.4
    numpy==1.20.1
    matplotlib==3.3.4
    gpflow==2.2.1

    This software was developed by employees of the National Institute of
    Standards and Technology (NIST), an agency of the Federal Government and
    is being made available as a public service. Pursuant to title 17 United
    States Code Section 105, works of NIST employees are not subject to
    copyright protection in the United States.  This software may be subject
    to foreign copyright.  Permission in the United States and in foreign
    countries, to the extent that NIST may hold copyright, to use, copy,
    modify, create derivative works, and distribute this software and its
    documentation without fee is hereby granted on a non-exclusive basis,
    provided that this notice and disclaimer of warranty appears in all
    copies.

    THE SOFTWARE IS PROVIDED 'AS IS' WITHOUT ANY WARRANTY OF ANY KIND, EITHER
    EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY
    WARRANTY THAT THE SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED
    WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND
    FREEDOM FROM INFRINGEMENT, AND ANY WARRANTY THAT THE DOCUMENTATION WILL
    CONFORM TO THE SOFTWARE, OR ANY WARRANTY THAT THE SOFTWARE WILL BE ERROR
    FREE.  IN NO EVENT SHALL NIST BE LIABLE FOR ANY DAMAGES, INCLUDING, BUT
    NOT LIMITED TO, DIRECT, INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES,
    ARISING OUT OF, RESULTING FROM, OR IN ANY WAY CONNECTED WITH THIS
    SOFTWARE, WHETHER OR NOT BASED UPON WARRANTY, CONTRACT, TORT, OR
    OTHERWISE, WHETHER OR NOT INJURY WAS SUSTAINED BY PERSONS OR PROPERTY OR
    OTHERWISE, AND WHETHER OR NOT LOSS WAS SUSTAINED FROM, OR AROSE OUT OF
    THE RESULTS OF, OR USE OF, THE SOFTWARE OR SERVICES PROVIDED HEREUNDER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "gMfzyJWuyhtm"
   },
   "source": [
    "# Infrastructure Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:35:34.082461Z",
     "start_time": "2023-03-10T21:35:34.060464Z"
    },
    "code_folding": [
     41,
     53,
     60,
     71,
     85,
     92,
     99,
     107
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1655590798058,
     "user": {
      "displayName": "Aaron Gilad Kusne",
      "userId": "07808887398243303382"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "HqHeqTtozIbm",
    "outputId": "246026a5-16f9-4a60-f592-956c512343c1"
   },
   "outputs": [],
   "source": [
    "%%writefile infrastructure_230101a.py\n",
    "\n",
    "# Generate a py file with useful functions.\n",
    "# These functions are gradually being moved to the other py files.\n",
    "# - match_rows_in_matrix\n",
    "# - normalize_each_row_by_sum\n",
    "# - tern2cart\n",
    "# - similarity_matrix\n",
    "# - sample_pool_sort\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "# import GPy\n",
    "\n",
    "from collections import namedtuple\n",
    "import simpy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gpflow.ci_utils import ci_niter\n",
    "# import GPy\n",
    "from scipy.spatial import distance as scipy_dist\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# ------------- Infrastructure code ----------- \n",
    "\n",
    "def match_rows_in_matrix(M, R, dist = 0, eps = 1E-3):\n",
    "    # find rows in matrix M that are near R within pairwise_distance of dist+eps \n",
    "    R = np.atleast_1d(R)\n",
    "    if len(M.shape) == 1:\n",
    "        M = M[:,None]\n",
    "    if len(R.shape) == 1:\n",
    "        R = R[:,None]\n",
    "    d = pairwise_distances(M, R)\n",
    "    a = d <= dist + eps\n",
    "    match_ = a.sum(axis = 1).nonzero()[0]\n",
    "    return match_\n",
    "\n",
    "def normalize_each_row_by_sum(X):\n",
    "    # normalize each row of a matrix by the row sum.\n",
    "    # useful for composition data\n",
    "    sumX = X.sum(axis = 1)[:,None]\n",
    "    X = X / np.tile(sumX,(1, X.shape[1]) )\n",
    "    return X\n",
    "\n",
    "def similarity_matrix(X):\n",
    "    # Compute similarity matrix W for XRD using cosine metric\n",
    "    sigma = 1\n",
    "    d_cos = squareform( pdist(X.squeeze(),'cosine') )\n",
    "    W = np.exp(-(d_cos**2) / (2*sigma**2))\n",
    "    return W\n",
    "\n",
    "def tern2cart(tern_composition):\n",
    "    # convert ternary data to cartesian coordinates.\n",
    "    t = normalize_each_row_by_sum(tern_composition) * 100\n",
    "    c = np.zeros((t.shape[0],2));\n",
    "    c[:,1] = t[:,1] * np.sin(60 * np.pi/180)/100\n",
    "    c[:,0] = t[:,0]/100 + c[:,1]*np.sin(30 * np.pi/180)/np.sin(60 * np.pi/180)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "_7LmBsIwzJCm"
   },
   "source": [
    "# AI Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:35:37.202924Z",
     "start_time": "2023-03-10T21:35:37.178891Z"
    },
    "code_folding": [
     65,
     105,
     150,
     167,
     179,
     198,
     224,
     295,
     307,
     317,
     330,
     339,
     343,
     439,
     446,
     455,
     465,
     471,
     477,
     489,
     515,
     531,
     538,
     551,
     571,
     606,
     624,
     633,
     640,
     658,
     668,
     707,
     738,
     753,
     759,
     764,
     770,
     777,
     784,
     798,
     803,
     817,
     892,
     932,
     945,
     952,
     978,
     992,
     1015,
     1025,
     1029,
     1033
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1655590799785,
     "user": {
      "displayName": "Aaron Gilad Kusne",
      "userId": "07808887398243303382"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "Y8S2XW7l5HYj",
    "outputId": "edcb556b-4e05-47c2-d4f2-ac31f4ca47a0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile network_ai_individual1_230101a.py\n",
    "# Generate py file to create the agent class 'control_ai'.\n",
    "# Abreviations:\n",
    "# msir - material system internal representation\n",
    "# spm - sample pool manager\n",
    "\n",
    "from infrastructure_230101a import match_rows_in_matrix, normalize_each_row_by_sum, tern2cart, similarity_matrix\n",
    "from global_variables_and_monitors_230101a import performance_monitor, mat_repository, agt_repository\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import simpy\n",
    "from tabulate import tabulate\n",
    "from collections import namedtuple\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from torch.distributions import constraints\n",
    "from torch.nn import Parameter, Softmax\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC, Predictive\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from gpflow.utilities import print_summary, set_trainable, to_default_float\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import distance as scipy_dist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import entropy, gmean\n",
    "from IPython.display import display\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from scipy.special import gamma\n",
    "from scipy.spatial import Voronoi\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "\n",
    "# example params\n",
    "# agt_params = {'AL_method':'entropy', 'x_range':np.asarray([2., 17., 0.1]), 'phase_mapping':None, 'combined_acquisition':False, 'comb_acq_weight':0.,\n",
    "#               'sample_pool_manager':spm_i,\n",
    "#               'central_mat_repo':None, 'central_agt_repo':None,\n",
    "#               'verbose':verbose}\n",
    "# agt_params['phase_mapping'] = {'num_clusters':3, 'stdiv_XRD':.1, 'stdiv_C':1}\n",
    "\n",
    "# example call:\n",
    "# control_ai(env, 'funcprop', uuid4().int, self.synth, self.meas_fp, agt_params)\n",
    "\n",
    "# Create AIs\n",
    "class control_ai:\n",
    "    def __init__(self, env, focus, unique_ID, synth, meas, params):\n",
    "        self.msir = None # initialize the material system internal representation (MSIR) to None \n",
    "        self.spm = params['sample_pool_manager']\n",
    "        self.meas = meas\n",
    "        self.synth = synth\n",
    "        self.env = env\n",
    "        self.running_proc = env.process(self.ai_run()) # tie the AI to ai_run in the environment.\n",
    "        self.focus = focus\n",
    "        self.unique_ID = unique_ID\n",
    "        self.cp_samples = []\n",
    "        self.cp_max_std = -1\n",
    "        self.curr_mean = []\n",
    "        self.curr_var = []\n",
    "        self.current_iteration_results = None\n",
    "        self.perf_monitor = None\n",
    "        self.BO_iter = 1\n",
    "        self.verbose = params['verbose']\n",
    "        self.save_figs = params['save_figs']\n",
    "        self.phase_mapping_AL_method = params['AL_PM_method'] #'entropy'\n",
    "        self.AL_BO_method = params['AL_BO_method']\n",
    "        self.phase_mapping_params = params['phase_mapping'] #{'num_clusters':2, 'stdiv_XRD':.1, 'stdiv_C':1}\n",
    "        self.changepoint = False\n",
    "        self.x_range = params['x_range']\n",
    "        self.combined_acquisition = False\n",
    "        self.meas_type = params['meas_type']\n",
    "        if params['use_cp']:\n",
    "            self.use_cp = True\n",
    "        if params['central_mat_repo'] is None:\n",
    "            self.mat_repo = mat_repository(env, self.spm)\n",
    "            self.use_coreg = False\n",
    "            # print('set to repo object')\n",
    "        else:\n",
    "            self.mat_repo = params['central_mat_repo']\n",
    "            self.agt_repo = params['central_agt_repo']\n",
    "            self.use_coreg = True\n",
    "            self.use_cp = True\n",
    "            if params['combined_acquisition']:\n",
    "                self.combined_acquisition = True\n",
    "                self.comb_acq_weight = params['comb_acq_weight']\n",
    "        \n",
    "    def ai_run(self): \n",
    "        # The primary function for the AI.\n",
    "        # Closed for loop for data collection, analysis, and decision making\n",
    "        \n",
    "        # if none of the samples in the sample pool have measurement, then measure two.\n",
    "        yield self.env.process(self.get_first_data_if_needed()) \n",
    "        yield self.env.timeout(10) # This gives enough time for all the initial measurements to be taken.\n",
    "        \n",
    "        for i in range(10): # loops\n",
    "            \n",
    "            # update msir and print\n",
    "            self.mat_repo.update_from_sample_pool()\n",
    "            yield self.env.timeout(1)\n",
    "            if self.verbose:\n",
    "                print(f'{self.focus} {str(self.unique_ID)[:5]} starts analysis with msir: ')\n",
    "                display(self.mat_repo.get_mat_data_all())\n",
    "            \n",
    "            # Run machine learning and active learning and store results\n",
    "            xx = np.arange(self.x_range[0],self.x_range[1],self.x_range[2])[:,None] # prediction points\n",
    "            xx = np.round_(xx,2)\n",
    "            comp_sorted_by_acq = self.analysis_prediction_and_active_learning(xx)  \n",
    "            yield self.env.timeout(1)\n",
    "            \n",
    "            # update history\n",
    "            self.update_perf_monitor()\n",
    "            force = False\n",
    "            redo = True\n",
    "            while redo:\n",
    "                # collect the new data, synthesize new sample if needed.\n",
    "                target_index, target_composition, measurement_result = yield self.env.process( self.collect_new_data_and_synth_if_needed(comp_sorted_by_acq, force) )\n",
    "                yield self.env.timeout(2)\n",
    "\n",
    "                # now that the sample is in msir, we can update its measurement value with the value measured.\n",
    "                redo = yield self.env.process(self.mat_repo.update_with_measurement(self.focus, target_composition, measurement_result))\n",
    "\n",
    "            yield self.env.timeout(1)\n",
    "                \n",
    "            # store results\n",
    "            self.store_meas_data(target_composition, measurement_result)\n",
    "            \n",
    "            # plot\n",
    "            plt.figure()\n",
    "            self.plot_current_iteration_results(xx, target_composition)\n",
    "       \n",
    "    # Getting data -----------------------------------\n",
    "    def get_first_data_if_needed(self):\n",
    "        self.mat_repo.update_from_sample_pool() # update mat repo from sample pool\n",
    "        repo = self.mat_repo.get_mat_data_all() # get the repo data\n",
    "        idx = [i for i in range(repo.shape[0]) if repo[self.focus].iloc[i] is not None]\n",
    "        if (len(idx) < 1):\n",
    "            composition_ = np.asarray([repo['composition'].iloc[i] for i in range(repo.shape[0]) if (repo[self.focus].iloc[i] is None)])\n",
    "            if self.verbose:\n",
    "                print(f'{self.focus} {str(self.unique_ID)[:5]}, composition list: {composition_}')\n",
    "            for i in range(2):\n",
    "                if self.verbose:\n",
    "                    print(f'{self.focus} {str(self.unique_ID)[:5]} getting data for c:{composition_[i]}')\n",
    "                measurement_result = yield self.env.process(self.meas.measure(composition_[i]))\n",
    "                redo = yield self.env.process(self.mat_repo.update_with_measurement(self.focus, composition_[i], measurement_result)) \n",
    "                if self.verbose:\n",
    "                    print(f'{self.focus} {str(self.unique_ID)[:5]} data for c:{composition_[i]}, meas: {measurement_result}')\n",
    "        yield self.env.timeout(1)\n",
    "        \n",
    "    def collect_new_data_and_synth_if_needed(self, comp_sorted_by_acq, force = False):  \n",
    "        yield self.env.timeout(1)\n",
    "        \n",
    "        target_comp = comp_sorted_by_acq[0]\n",
    "        # synthesize if needed.\n",
    "        target_index = yield self.env.process( self.synth_if_needed(target_comp, force) )\n",
    "               \n",
    "        # measure\n",
    "        measurement_result = yield self.env.process( self.collect_data_for_target_sample(target_comp, target_index) )\n",
    "        \n",
    "        return target_index, target_comp, measurement_result\n",
    "    \n",
    "    def collect_data_for_target_sample(self, target_composition, target_index):\n",
    "        if type(target_composition) is np.ndarray:\n",
    "            target_composition = target_composition.item()\n",
    "        self.mat_repo.update_from_sample_pool() # update mat repo from sample pool\n",
    "        # find which sample has target_composition\n",
    "        with self.meas.sample_measure.request() as req:\n",
    "            yield req # request measurement access\n",
    "            print(f'{self.focus} {str(self.unique_ID)[:5]} requested measurement of c:{target_composition} at {self.env.now}')\n",
    "            if self.verbose:\n",
    "                print(spm.list_sample_pool_items())\n",
    "            self.meas.target_composition = target_composition\n",
    "            self.mat_repo.update_from_sample_pool() # update msir again\n",
    "            # hand over control to the measurement instrument.\n",
    "            measurement_result = yield self.env.process(self.meas.measure(target_composition)) # request measurement\n",
    "            print(f'{self.focus} {str(self.unique_ID)[:5]} received measurement result for c:{target_composition} at {self.env.now}')\n",
    "            self.mat_repo.update_from_sample_pool() # update msir\n",
    "\n",
    "        return measurement_result\n",
    "    \n",
    "    def synth_if_needed(self, target_comp, force):\n",
    "        if type(target_comp) is np.ndarray:\n",
    "            target_comp = target_comp.item()\n",
    "        self.mat_repo.update_from_sample_pool() # update msir again\n",
    "        compositions_, sample_indices_ = self.mat_repo.get_compositions_and_sample_indices() # get the compositions of the samples in msir\n",
    "        # match_index indicates which samples in msir match the target_composition\n",
    "        match_index = match_rows_in_matrix(compositions_, target_comp)\n",
    "\n",
    "        if not match_index.size or force: # if the target_composition is not in the pool\n",
    "            with self.synth.sample_synthesis.request() as req: # synthesize the sample\n",
    "                yield req # request synthesis access\n",
    "                print(f'{self.focus} {str(self.unique_ID)[:5]} requested synthesis of c:{target_comp} at {self.env.now}')\n",
    "                # synthesize sample\n",
    "                self.synth.target_composition = target_comp\n",
    "                # request that the sample is synthesized and wait until finished.\n",
    "                # new sample is added to the sample_pool with the index new_sample_index\n",
    "                new_sample_index = yield self.env.process(self.synth.synthesis(target_comp))\n",
    "                print(f'{self.focus} {str(self.unique_ID)[:5]} synthesis complete of c:{target_comp} at {self.env.now}')\n",
    "                # add new sample to internal representation\n",
    "            self.mat_repo.update_from_sample_pool()\n",
    "            target_index = new_sample_index # if synthesized, then the sample to measure is indicated by new_sample_index\n",
    "        else:\n",
    "            target_index = sample_indices_[match_index[0]] # if the sample is in the sample pool, then select that sample to be measured.\n",
    "        return target_index\n",
    "    \n",
    "    # ML ----------------------------------------------\n",
    "    def analysis_prediction_and_active_learning(self, X_full):\n",
    "        data = self.get_training_data()\n",
    "        if self.verbose:\n",
    "            print(f'{self.focus}{str(self.unique_ID)[:5]} got training data')\n",
    "        \n",
    "        if self.use_coreg and self.use_cp:\n",
    "            # print('clustering')\n",
    "            cl, Ux = self.phase_mapping(data[2], data[3])\n",
    "            # print('coreg:prediction') \n",
    "            X_FP=data[0]; Y_FP=data[1]; X_XRD=data[2]; Y_XRD=data[3]\n",
    "    \n",
    "            Fmu, Fvar, Pmu, Pvar, cp = self.bi_csp_full_Bayesian(X_full, X_XRD, cl, X_FP, Y_FP) # assumes num_clusters = 3\n",
    "            self.cp_samples = cp\n",
    "            self.curr_mean = Fmu.flatten()\n",
    "            self.curr_var = Fvar.flatten()\n",
    "            if self.cp_max_std < np.std(cp,axis=0).max():\n",
    "                self.cp_max_std = np.std(cp,axis=0).max()\n",
    "            if self.verbose:\n",
    "                print('cp: clustered, prediction, AL,cp:' + self.focus[5:])\n",
    "            if self.focus=='funcprop':\n",
    "                training_data = (data[0], data[1])\n",
    "                alpha = self.AL_BO(Fmu, Fvar, method=self.AL_BO_method, change_points = cp, Xpred = X_full)\n",
    "                self.BO_iter = self.BO_iter + 1\n",
    "                pred = Fmu; var_or_Cov = Fvar;\n",
    "            elif self.focus=='structure':\n",
    "                training_data = (data[2], cl)\n",
    "                alpha = self.AL_PM(Pmu, Pvar, method=self.phase_mapping_AL_method, x = (data[2], X_full))\n",
    "                pred = Pmu\n",
    "            \n",
    "                var_or_Cov = [] #np.sum(Pvar, axis=1).flatten();\n",
    "            if self.combined_acquisition:\n",
    "                # print('sizes to store in agt repo:',alpha.shape, pred.shape)\n",
    "                indep_var = np.arange(self.x_range[0],self.x_range[1],self.x_range[2]).flatten()\n",
    "                self.agt_repo.update_record(self.unique_ID, indep_var, alpha, pred)\n",
    "    \n",
    "        elif self.focus == 'funcprop': # if AI name is Mag, then perform GPR and GP-UCB Bayesian optimization\n",
    "            # print('funcprop:starting GPR')\n",
    "            training_data = (data[0], data[1])\n",
    "            mean, var, Cov = self.func_prop_regression_gpflow_with_error_catching(data[0], data[1], X_full)\n",
    "            # print('funcprop:AL')\n",
    "            alpha = self.AL_BO(mean.flatten(), var.flatten(), method=self.AL_BO_method)\n",
    "            self.BO_iter = self.BO_iter + 1\n",
    "            pred = mean; var_or_Cov = Cov;\n",
    "            if self.verbose:\n",
    "                print('funcprop: GPR, AL')\n",
    "            \n",
    "        elif self.focus == 'structure': # if AI named xrd, perform GPC and active learning (variance maximization)\n",
    "            # print('structure:phase mapping')\n",
    "            cl, Ux = self.phase_mapping(data[0], data[1])\n",
    "            training_data = (data[0], cl)\n",
    "            \n",
    "            if self.use_cp:\n",
    "                U, U_var = self.bi_cs(data[0], cl, X_full)\n",
    "            else: \n",
    "                # print('structure:prediction')\n",
    "                U, U_var = self.phase_map_extrapolate(data[0], Ux, X_full)\n",
    "                # print('structure:AL')\n",
    "            alpha = self.AL_PM(U, U_var, method=self.phase_mapping_AL_method, x = (data[0], X_full))\n",
    "            pred = U.copy()\n",
    "            var_or_Cov = [] #np.sum(U_var, axis=1).flatten();\n",
    "            if self.verbose:\n",
    "                print('structure: phase map, prediction, AL')\n",
    "            \n",
    "        # selecting next sample\n",
    "        query, comp_sorted_by_acq = self.pick_next_measurement(alpha, X_full)\n",
    "        \n",
    "        # store results\n",
    "        self.store_ML_data(pred, var_or_Cov, alpha, comp_sorted_by_acq, training_data)\n",
    "        \n",
    "        return comp_sorted_by_acq\n",
    "    \n",
    "    def store_ML_data(self, pred, var_or_Cov, acquisition_function, composition_sorted_by_acq, training_data):\n",
    "        if self.current_iteration_results is None:\n",
    "            temp = {'prediction':[pred],'var_or_Cov':[var_or_Cov],'composition_sorted_by_acq':[composition_sorted_by_acq], \\\n",
    "                'acquisition_function':[acquisition_function],'training_data':[training_data]}\n",
    "            self.current_iteration_results = temp\n",
    "        else:\n",
    "            self.current_iteration_results['prediction'].append(pred)\n",
    "            self.current_iteration_results['var_or_Cov'].append(var_or_Cov)\n",
    "            self.current_iteration_results['composition_sorted_by_acq'].append(composition_sorted_by_acq)\n",
    "            self.current_iteration_results['acquisition_function'].append(acquisition_function)\n",
    "            self.current_iteration_results['training_data'].append(training_data)\n",
    "            \n",
    "    def store_meas_data(self, x, y):\n",
    "        if 'x' in self.current_iteration_results:\n",
    "            x0 = self.current_iteration_results['x']\n",
    "            y0 = self.current_iteration_results['y']\n",
    "            self.current_iteration_results['x'] = np.concatenate((x0,x.flatten()[None,:]),axis=0)\n",
    "            self.current_iteration_results['y'] = np.concatenate((y0,y.flatten()[None,:]),axis=0)\n",
    "        else:\n",
    "            self.current_iteration_results['x'] = x.flatten()[None,:]\n",
    "            self.current_iteration_results['y'] = y.flatten()[None,:]\n",
    "            \n",
    "    def update_perf_monitor(self):\n",
    "        yy = self.current_iteration_results['prediction'][-1]\n",
    "        training_data = self.current_iteration_results['training_data'][-1]\n",
    "        if self.focus == 'funcprop':\n",
    "            if training_data[1].flatten().shape[0] == 2:\n",
    "                entry = training_data[1].flatten()\n",
    "            else:\n",
    "                entry = np.atleast_1d( np.max(training_data[1]) )\n",
    "            self.perf_monitor = performance_monitor.history_tracking(self.perf_monitor, entry)\n",
    "        elif self.focus == 'structure':\n",
    "            yy = np.argmax(yy, axis = 1)\n",
    "            self.perf_monitor = performance_monitor.history_tracking(self.perf_monitor, yy.squeeze()[None,:])\n",
    "    \n",
    "    def get_training_data(self):\n",
    "        if self.use_coreg:\n",
    "            dataFP = self.mat_repo.get_mat_data('funcprop')\n",
    "            dataXRD = self.mat_repo.get_mat_data('structure')\n",
    "            data = (dataFP[0], dataFP[1], dataXRD[0], dataXRD[1])\n",
    "        else:\n",
    "            data = self.mat_repo.get_mat_data(self.focus)\n",
    "        return data\n",
    "    \n",
    "    def get_training_data_one_source(self):\n",
    "        data = self.mat_repo.get_mat_data(self.focus)\n",
    "        return data\n",
    "\n",
    "    def pick_next_measurement(self, alpha, X_full):\n",
    "        # for the compositions sorted by aquisition values\n",
    "        # remove those that are within 1E-2 of compositions in msir with measurement data.\n",
    "        # This is done for the desired measurement type\n",
    "        # print(f'pick next measurement, acq: {composition_sorted_by_acq[0:5].flatten()}')\n",
    "        \n",
    "        # !!!!!!!!!!!!! IF COMBINING ACQUISITION FUNCTIONS !!!!!!!!!!!!!!!!\n",
    "        # Second condition here, only uses joint acq if the target is materials opt.\n",
    "        if self.combined_acquisition and self.focus == 'funcprop':\n",
    "            if self.verbose:\n",
    "                print('inside combined acq')\n",
    "                display(self.agt_repo.get_repo())\n",
    "            # data_mean := {'indep_var': indep_var, 'acq_mean': acq_mean, 'pred_mean': 'Not used', \\\n",
    "            #          'mean_fp':mean_fp,'data_fp':data_fp,'mean_st':mean_st,'data_st':data_st}\n",
    "            data_mean, data_all = self.agt_repo.get_other_records(self.unique_ID)\n",
    "            stuff = [data_mean['mean_fp'], data_mean['data_fp'], data_mean['mean_st'], data_mean['data_st']]\n",
    "\n",
    "            alpha_before = alpha.flatten().copy()           \n",
    "            \n",
    "            if data_mean['mean_st'] is not None:\n",
    "                alpha_pm = normalize(data_mean['mean_st'].flatten())\n",
    "                alpha_fp = normalize(alpha_before.flatten())\n",
    "                \n",
    "                cp_std = np.std(self.cp_samples , axis = 0)\n",
    "\n",
    "                pm_weight = np.minimum(cp_std.max(),2.) / 2.\n",
    "                fp_weight = 1 - pm_weight\n",
    "                print('pm_weight,fp_weight', pm_weight, fp_weight)\n",
    "                alpha = pm_weight*alpha_pm + fp_weight*alpha_fp\n",
    "                \n",
    "                if self.current_iteration_results is not None:\n",
    "                    self.current_iteration_results['acquisition_function'].append(alpha.flatten())\n",
    "            else:\n",
    "                alpha = alpha_before      \n",
    "            \n",
    "            if data_mean['acq_mean'] is not None:\n",
    "                \n",
    "                print\n",
    "                plt.figure(figsize = (12,2))\n",
    "                plt.subplot(1,3,1)\n",
    "                plt.plot(normalize(alpha_before))\n",
    "                plt.title(f'{self.focus} {str(self.unique_ID)[:5]} alpha before')\n",
    "                plt.subplot(1,3,2)\n",
    "                plt.plot(normalize(data_mean['data_st']).T)\n",
    "                plt.title('st data')\n",
    "                plt.subplot(1,3,3)\n",
    "                plt.plot(normalize(alpha))\n",
    "                plt.title('alpha after')              \n",
    "                \n",
    "                if self.save_figs:\n",
    "                    dn = r'G:\\\\My Drive\\\\Research\\\\jupyter\\\\Networked ML\\\\figs\\\\'\n",
    "                    title_str = self.focus + str(self.unique_ID)[:5]\n",
    "                    plt.savefig(dn + 'comb_acq_' + title_str + '_at_' + str(self.env.now) + '.svg', format='svg')\n",
    "                    \n",
    "                plt.show()\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!  \n",
    "    \n",
    "        comp_sorted_by_acq = X_full[np.argsort(-alpha.flatten())].flatten()\n",
    "        comps2compare = np.empty((0))\n",
    "        \n",
    "        # composition for samples already measured.\n",
    "        temp, _ = self.get_training_data_one_source()\n",
    "        comps2compare = np.concatenate((comps2compare, temp.flatten()), axis = 0)\n",
    "        \n",
    "        # create list of samples currently being made\n",
    "        synth_list = self.spm.synth_list.copy()\n",
    "        if synth_list:\n",
    "            temp = np.asarray(synth_list).flatten()\n",
    "            comps2compare = np.concatenate((comps2compare, temp), axis = 0)\n",
    "            \n",
    "        # samples currently being measured\n",
    "        lent_list = self.spm.lent_list.copy()\n",
    "        if lent_list:\n",
    "            comps_lent = np.asarray(lent_list).flatten()\n",
    "            comps_lent_purpose = self.spm.lent_purpose.copy()\n",
    "            if comps_lent.flatten().shape[0] != len(comps_lent_purpose):\n",
    "                print('wtf: comps_lent:', comps_lent, 'purpose:', comps_lent_purpose)\n",
    "            # If the sample is currently being measured for the same purpose, ignore.\n",
    "            kp = [idx for idx, p in enumerate(comps_lent_purpose) if p == self.focus]\n",
    "            if kp:              \n",
    "                temp = comps_lent[kp]\n",
    "                    \n",
    "                # print('sizechallenge',lent_list, comps_lent, comps2compare.shape, temp.shape)\n",
    "                comps2compare = np.concatenate((comps2compare, temp.flatten()), axis = 0)\n",
    "    \n",
    "        drop_idx = match_rows_in_matrix(comp_sorted_by_acq, comps2compare)\n",
    "        new_comp_list = np.delete(comp_sorted_by_acq.flatten(), drop_idx, axis = 0)\n",
    "        \n",
    "        query = np.atleast_1d(new_comp_list[0])\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'orig:{comp_sorted_by_acq[0:5].flatten()}, new:{new_comp_list[0:5].flatten()}')\n",
    "        return np.round_(query, decimals=2), np.round_(new_comp_list, decimals=2)\n",
    "\n",
    "    # Phase Mapping -----------------------\n",
    "    def similarity_matrix(self, Y, stdY):\n",
    "        # Kernel used for spectral clustering,\n",
    "        # print('sim',X.shape, Y.shape)\n",
    "        d_Y = pairwise_distances(Y, metric = 'cosine')\n",
    "        K_Y = np.exp(- d_Y**2 / stdY**2)\n",
    "        return K_Y\n",
    "    \n",
    "    def phase_mapping(self, X, Y):\n",
    "        stdX = self.phase_mapping_params['stdiv_C']\n",
    "        stdY = self.phase_mapping_params['stdiv_XRD']\n",
    "        num_clusters = self.phase_mapping_params['num_clusters']\n",
    "        # various methods for performing phase mapping\n",
    "        cl, Ux = self.phase_mapping_Kmedoids(X, Y, num_clusters)\n",
    "        # cl, Ux = self.phase_mapping_ground_truth(X, num_clusters)\n",
    "        return cl, Ux\n",
    "\n",
    "    def phase_mapping_Kmedoids(self, X, Y, num_clusters):\n",
    "        if X.shape[0] <= num_clusters:\n",
    "            cluster_prob = np.eye(X.shape[0])\n",
    "            cl = np.argmax(cluster_prob, axis=1).flatten()\n",
    "        else:\n",
    "            cl = KMedoids(n_clusters=num_clusters, metric = 'cosine', method = 'pam', random_state=0).fit(Y).labels_\n",
    "            cl = cl.flatten()\n",
    "            cluster_prob = self.hard_labels_to_Ux(cl,num_clusters)\n",
    "        return cl, cluster_prob    \n",
    "    \n",
    "    def hard_labels_to_Ux(self,cl,N):\n",
    "        Ux = np.zeros((cl.flatten().shape[0],N)).astype(int)\n",
    "        idx = np.arange(0,cl.flatten().shape[0])\n",
    "        Ux[idx,cl] = 1\n",
    "        return Ux\n",
    "\n",
    "    def phase_map_extrapolate(self, X, Ux, X_full):\n",
    "        # extrapolate phase map results\n",
    "        cl = np.argmax(Ux, axis = 1).flatten()\n",
    "        U_full, U_var = self.phase_mapping_classification_with_error_catching(X, Ux, cl, X_full)\n",
    "        return U_full, U_var\n",
    "    \n",
    "    def phase_mapping_classification_with_error_catching(self, X, Ux, cl, X_full):\n",
    "        num_clusters = np.unique(cl).shape[0]\n",
    "        N = X_full.shape[0]\n",
    "        # If there is only 1 cluster, set the output values.\n",
    "        if num_clusters == 1:\n",
    "            mean = np.ones((N,1))\n",
    "            var = np.zeros((N,1))\n",
    "        else:\n",
    "            counter = 0\n",
    "        mean, var, m = self.phase_map_classification_gpflow(X, Ux, cl, X_full)\n",
    "        return mean, var\n",
    "\n",
    "    def phase_map_classification_gpflow(self, X, Ux, cl, X_full):\n",
    "        cl = cl.flatten()[:,None]\n",
    "        # Apply GPC to extrapolate phase region labels\n",
    "        C = 3\n",
    "        data = (X, cl) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "        kernel = gpflow.kernels.Matern32() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "        # Robustmax Multiclass Likelihood\n",
    "        invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "        likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "        m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "        #m.likelihood.variance.assign(0.05)\n",
    "        #p = m.likelihood.variance\n",
    "        #m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.01), f64(0.1)) )    \n",
    "\n",
    "        opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "        opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "        f = m.predict_f(X_full) # what is the (non-squeezed) probabilistic function for extrapolating class labels over full XY coordinates\n",
    "        y = m.predict_y(X_full) # what is the Poisson process for the full XY coordinates\n",
    "        f_mean = f[0].numpy() # mean of f\n",
    "        f_var = f[1].numpy() # variance of f\n",
    "        y_mean = y[0].numpy() # mean of y\n",
    "        y_var = y[1].numpy() # variance of y.\n",
    "        return y_mean, y_var, m\n",
    "    \n",
    "    # ------- Regression -------------\n",
    "    def func_prop_regression_gpflow(self, X, Y, X_full, minimize_method):\n",
    "        #GPflow GPR for one scalar property.\n",
    "        k = gpflow.kernels.SquaredExponential(lengthscales = [1])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "        data = (tf.convert_to_tensor(X), tf.convert_to_tensor(Y.flatten()[:,None]))\n",
    "        m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(Y.mean())) # set up GPR model\n",
    "\n",
    "        m.likelihood.variance.assign(0.01)\n",
    "        p = m.likelihood.variance\n",
    "        m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.1)) )    \n",
    "\n",
    "        opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "        opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = minimize_method, options=dict(maxiter=100))  # run optimization\n",
    "        temp_mean, temp_var = m.predict_f(X_full) # compute the mean and variance for the other samples in the phase region\n",
    "        _, temp_Cov = m.predict_f(X_full, full_cov = True)\n",
    "        return temp_mean.numpy(), temp_var.numpy(), temp_Cov.numpy().squeeze()\n",
    "\n",
    "    def func_prop_regression_gpflow_with_error_catching(self, X, Y, X_full):\n",
    "        count = 0\n",
    "        methods = ['tnc', 'BFGS', 'L-BFGS-B']\n",
    "        minimize_method = methods[count]\n",
    "        mean, var, Cov = self.func_prop_regression_gpflow(X, Y, X_full, minimize_method)\n",
    "        return mean, var, Cov \n",
    "    \n",
    "    def bi_csp_1D_full_Bayesian_analysis(self,Xpred, xs, ys, xf, yf, num_regions, numsteps, JIT = True):\n",
    "        # Can use samples of change_point to define uncertainty.\n",
    "        # can bin changepoint to make this faster and iterate over bins.\n",
    "        data = [xs, ys, xf, yf, Xpred, num_regions]\n",
    "        ker = NUTS(model_1D_joint_change_points_w_bounds_Bayesian, jit_compile=JIT, ignore_jit_warnings=True, max_tree_depth=3)\n",
    "        posterior = MCMC(ker, num_samples=numsteps, warmup_steps=100)\n",
    "        posterior.run(data);\n",
    "\n",
    "        samples = {k: v.detach().cpu().numpy() for k, v in posterior.get_samples().items()}\n",
    "        predictive = Predictive(model_1D_joint_change_points_w_bounds_Bayesian, posterior.get_samples())(data)\n",
    "\n",
    "        return samples, predictive\n",
    "    \n",
    "    def bi_csp_full_Bayesian(self, Xpred, xs, ys, xf, yf):\n",
    "        # Can use samples of change_point to define uncertainty.\n",
    "        # can bin changepoint to make this faster and iterate over bins.\n",
    "        num_regions = 3\n",
    "        numsteps = 500\n",
    "        \n",
    "        data = [xs, ys, xf, yf, Xpred, num_regions]\n",
    "        pyro.clear_param_store()\n",
    "        samples, predictive = self.bi_csp_1D_full_Bayesian_analysis(Xpred, xs, ys, xf, yf, num_regions, numsteps, JIT = False)\n",
    "\n",
    "        region_prob, fhat_mean, fhat_std, cp = self.bi_csp_1D_full_Bayesian_predict(Xpred, xf, yf, samples, predictive, num_regions, eps = 1E-6)\n",
    "\n",
    "        # assumes only one functional property\n",
    "        Fmu = fhat_mean.flatten()[:,None]\n",
    "        Fvar = fhat_std.flatten()[:,None]\n",
    "        Pmu = region_prob\n",
    "        Pvar = [] \n",
    "        \n",
    "        return Fmu, Fvar, Pmu, Pvar, cp\n",
    "    \n",
    "    def bi_csp_1D_full_Bayesian_predict(self, Xpred, xf, yf, samples, predictive, num_regions, eps = 1E-6):\n",
    "        Nnew = Xpred.shape[0]\n",
    "        Nsamples_GPR = 10\n",
    "        eps = 1E-6\n",
    "        cp_samples = samples['changepoint_'] \n",
    "\n",
    "        if num_regions == 1:\n",
    "            cp_ =cp_samples.squeeze(-1)\n",
    "        else:\n",
    "            cp_ = np.sort(cp_samples,axis=1)\n",
    "        cp = cp_.mean(axis=0)\n",
    "        region_prob = cp_samples_to_class_prob(cp_, Xpred)\n",
    "\n",
    "        # !!!!! WE SAMPLE FROM MVN using mean and COV and output the sample\n",
    "        # ! 1 sample gives good approx to results when using 10 samples.\n",
    "        fhat_Xpred = np.zeros((samples['changepoint_'].shape[0]*Nsamples_GPR,Xpred.shape[0]))\n",
    "        for i in range(samples['changepoint_'].shape[0]):\n",
    "            mean, cov, _, iseg = gpr_piecewise_forward_w_mean(Xpred, xf, yf, samples['changepoint_'][i,:], \\\n",
    "                                                        samples['gp_var'][i,:], samples['gp_lengthscale'][i,:], \\\n",
    "                                                        samples['gp_noise'][i], predictive['prior_means'][i])\n",
    "            mean_Xpred = torch.cat(mean)\n",
    "            # print('mean_Xpred:', mean_Xpred.flatten())\n",
    "            cov_Xpred = torch.zeros((Xpred.shape[0], Xpred.shape[0]))\n",
    "            for j in range(len(iseg)):\n",
    "                seg_curr = np.argwhere(iseg[j]).flatten()\n",
    "                cov_Xpred[seg_curr[:,None],seg_curr] = to_torch(cov[j])\n",
    "            temp = cov_Xpred + torch.eye(Nnew) * eps\n",
    "            idx = torch.arange(Nsamples_GPR*i,Nsamples_GPR*(i+1))\n",
    "            fhat_Xpred[idx,:] = dist.MultivariateNormal(mean_Xpred.flatten(), cov_Xpred + torch.eye(Nnew) * eps).sample((Nsamples_GPR,))\n",
    "\n",
    "        fhat_mean = fhat_Xpred.mean(axis=0)\n",
    "        fhat_std = fhat_Xpred.std(axis=0)\n",
    "\n",
    "        return region_prob, fhat_mean, fhat_std, cp_\n",
    "\n",
    "    def bi_cs(self, xs, ys, X_pred):\n",
    "        # Can use samples of change_point to define uncertainty.\n",
    "        # can bin changepoint to make this faster and iterate over bins.\n",
    "        data = [xs, ys, X_pred]\n",
    "\n",
    "        ker = NUTS(model_pm_1D_CP_w_bounds, jit_compile=False, ignore_jit_warnings=True, max_tree_depth=3)\n",
    "        posterior = MCMC(ker, num_samples=100, warmup_steps=10)\n",
    "        posterior.run(data);\n",
    "        \n",
    "        s = {k: v.detach().cpu().numpy() for k, v in posterior.get_samples().items()}\n",
    "        cp_ = np.sort(s['changepoint_'],axis=1)\n",
    "\n",
    "        Pmu = cp_samples_to_class_prob(cp_, X_pred)\n",
    "        Pvar = []\n",
    "        \n",
    "        return Pmu, Pvar\n",
    "    \n",
    "    # Active Learning ----------------\n",
    "    def AL_BO(self, mean, var, method='UCB', change_points = None, Xpred = None):\n",
    "        if method=='UCB':\n",
    "            Dsize = mean.shape[0]\n",
    "            alpha = self.gp_ucb(Dsize, mean, var, BO_lambda = .1)\n",
    "        elif method=='UCB+CP':\n",
    "            Dsize = mean.shape[0]\n",
    "            alpha = self.gp_ucb_cp(Dsize, mean, var, change_points, Xpred, BO_lambda = .1)            \n",
    "        return alpha\n",
    "\n",
    "    def gp_ucb(self, Dsize, mean, var, BO_lambda = .1):\n",
    "        # compute utility of samples using 'gaussian process - upper confidence bound' and then select sample with highest utility\n",
    "        BO_beta = 2*np.log(Dsize * (self.BO_iter**2) * (np.pi**2) / (6 * BO_lambda) ) # GP-UCB\n",
    "        BO_alpha = mean + np.sqrt(BO_beta) * np.sqrt(var)\n",
    "        self.BO_iter = self.BO_iter + 1\n",
    "        return BO_alpha\n",
    "    \n",
    "    def gp_ucb_cp(self, Dsize, mean, var, cp, Xpred, BO_lambda = .1):\n",
    "        # compute utility of samples using 'gaussian process - upper confidence bound' and then select sample with highest utility\n",
    "        d = pairwise_distances(Xpred.flatten()[:,None], cp.flatten()[:,None])\n",
    "        d = np.min(d, axis = 1)\n",
    "        r = np.exp(-.5*d**2 / 1).flatten()\n",
    "        if len(var.shape) == 2:\n",
    "            r = r.flatten()[:,None]\n",
    "        \n",
    "        BO_beta = 2*np.log(Dsize * (self.BO_iter**2) * (np.pi**2) / (6 * BO_lambda) ) # GP-UCB\n",
    "        BO_alpha = mean + np.sqrt(BO_beta) * np.sqrt(var)\n",
    "        self.BO_iter = self.BO_iter + 1\n",
    "        # print('UCB:', BO_alpha.shape, BO_alpha.flatten())\n",
    "        # print('r:',r.shape, r.flatten())\n",
    "        BO_alpha = BO_alpha + r\n",
    "        # print('UCB+r:', BO_alpha.shape, BO_alpha.flatten())\n",
    "        \n",
    "        return BO_alpha\n",
    "\n",
    "    def AL_PM(self, U, U_var, method='entropy', x=None):\n",
    "        # update process bar\n",
    "        # active learning schemes applied to phase mapping objective.\n",
    "        if method == 'entropy':\n",
    "            alpha = entropy(U, axis=1).flatten()\n",
    "        return alpha \n",
    "        \n",
    "    # Plot ----------------------------------\n",
    "    \n",
    "    # plot GP regression results\n",
    "    def plot_gpr(self, X, m, C, training_points=None, acq=None, query=None, history=None):\n",
    "        X, m, C = to_numpy([X, m, C])\n",
    "        if len(C.squeeze().shape) == 2:\n",
    "            C = np.diag(C)\n",
    "        # print('inside gpr plot',X.shape,m.shape,C.shape)\n",
    "        plt.figure(figsize = (12,2))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.fill_between(X.flatten(), m.flatten() - 1.96*np.sqrt(C.flatten()), m.flatten() + 1.96*np.sqrt(C.flatten()), alpha=0.5);\n",
    "        plt.plot(X, m, \"-\");\n",
    "        title_str = self.focus + str(self.unique_ID)[:5] \n",
    "        if self.use_coreg:\n",
    "            title_str = title_str + ' coreg'\n",
    "        if self.combined_acquisition:\n",
    "            title_str = title_str + ' comb_acq'\n",
    "        plt.title(title_str + ' at ' + str(self.env.now))\n",
    "        if training_points is not None:\n",
    "            X_, Y_ = training_points\n",
    "            plt.plot(X_, Y_, \"kx\", mew=2);\n",
    "        if acq is not None:\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.plot(X, acq)\n",
    "        if query is not None:\n",
    "            minTS = np.min(acq) \n",
    "            maxTS = np.max(acq) \n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot([query, query], [minTS, maxTS], 'm');\n",
    "        if history is not None:\n",
    "            plt.subplot(1,3,3)\n",
    "            simple_regret = performance_monitor.simple_regret(history, self.meas_type)\n",
    "            plt.plot(simple_regret);\n",
    "            # plt.ylabel('%')\n",
    "\n",
    "        if self.save_figs:\n",
    "            dn = r'G:\\\\My Drive\\\\Research\\\\jupyter\\\\Networked ML\\\\figs\\\\'\n",
    "            plt.savefig(dn + 'gpr_' + title_str + '_at_' + str(self.env.now) + '.svg', format='svg')\n",
    "    \n",
    "        plt.show();\n",
    "        \n",
    "    # plot GP classification results\n",
    "    def plot_gpc(self, X, label_curves, var_or_Cov, training_points=None, acq=None, query=None, history=None):\n",
    "        plt.figure(figsize = (12,2))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.plot(X, label_curves)\n",
    "        #plt.ylim([-.1, 1.1])\n",
    "        plt.plot([query, query],[0,1],'m');\n",
    "        title_str = self.focus + str(self.unique_ID)[:5]\n",
    "        if self.use_coreg:\n",
    "            title_str = title_str + ' coreg'\n",
    "        if self.combined_acquisition:\n",
    "            title_str = title_str + ' comb_acq'\n",
    "        plt.title(title_str + ' at ' + str(self.env.now))\n",
    "        \n",
    "        if training_points is not None:\n",
    "            X_, Y_ = training_points\n",
    "            plt.plot(X_,Y_/2, 'kx', mew=2);\n",
    "            \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.plot(X, acq)\n",
    "        plt.title('acq')\n",
    "        if history is not None:\n",
    "            plt.subplot(1,3,3)\n",
    "            fmi = performance_monitor.phase_mapping_performance(history, self.x_range)\n",
    "            plt.plot(fmi);\n",
    "        \n",
    "        if self.save_figs:\n",
    "            dn = r'G:\\\\My Drive\\\\Research\\\\jupyter\\\\Networked ML\\\\figs\\\\'\n",
    "            plt.savefig(dn + 'gpc_' + title_str + '_at_' + str(self.env.now) + '.svg', format='svg')\n",
    "\n",
    "        plt.show();\n",
    "        \n",
    "    def plot_current_iteration_results(self, xx, query):\n",
    "        yy = self.current_iteration_results['prediction'][-1]\n",
    "        var_or_Cov = self.current_iteration_results['var_or_Cov'][-1]\n",
    "        training_data = self.current_iteration_results['training_data'][-1]\n",
    "        acq = self.current_iteration_results['acquisition_function'][-1]\n",
    "        # print(xx.shape, yy.shape, var_or_Cov.shape, acq.shape)\n",
    "        if self.focus == 'funcprop':\n",
    "            self.plot_gpr(xx, yy, var_or_Cov, training_points=training_data, acq = acq, query = query, history = self.perf_monitor)\n",
    "        elif self.focus == 'structure':\n",
    "            self.plot_gpc(xx, yy, var_or_Cov, training_points=training_data, acq = acq, query = query, history = self.perf_monitor)\n",
    "\n",
    "        \n",
    "# ------------ Support Functions ------------------------\n",
    "\n",
    "# Support -----------------------------\n",
    "def to_numpy(v):\n",
    "    for i in range(len(v)):\n",
    "        if type(v[i]) is not np.ndarray:\n",
    "            v[i] = v[i].numpy()\n",
    "    return v\n",
    "\n",
    "def to_torch(v):\n",
    "    if not torch.is_tensor(v):\n",
    "        v = torch.tensor(v)\n",
    "    return v\n",
    "\n",
    "def make_2d(v):\n",
    "    for i in range(len(v)):\n",
    "        if len(v[i].shape == 1):\n",
    "            v[i] = v[i][:,None]\n",
    "    return v\n",
    "    \n",
    "def change_points_to_labels(cp, X):\n",
    "    if type(X) is not np.ndarray:\n",
    "        X = X.numpy()\n",
    "    cp = np.sort(cp)\n",
    "    cl = np.zeros((X.shape[0])).astype(np.compat.long)\n",
    "    N = cp.shape[0] # N = 3\n",
    "    for i in np.arange(0, N):\n",
    "        if i < N-1:\n",
    "            idx = np.logical_and(X > cp[i], X < cp[i+1])\n",
    "        elif i == N-1:\n",
    "            idx = X > cp[i]\n",
    "        cl[idx.flatten()] = i+1\n",
    "    return cl\n",
    "\n",
    "def change_points_to_labels_torch(cp, X):\n",
    "    if type(X) is np.ndarray:\n",
    "        X = torch.tensor(X)\n",
    "    cp,_ = torch.sort(cp)\n",
    "    cl = torch.zeros((X.shape[0])).long()\n",
    "    N = cp.shape[0] # N = 3\n",
    "    for i in range(0, N):\n",
    "        if i < N-1:\n",
    "            idx = torch.logical_and(X > cp[i], X < cp[i+1])\n",
    "        elif i == N-1:\n",
    "            idx = X > cp[i]\n",
    "        cl[idx.flatten()] = i+1\n",
    "    return cl\n",
    "    \n",
    "def one_hot_np(v, cp, X):\n",
    "    oh = np.zeros((v.shape[0], cp.shape[0] + 1))\n",
    "    oh[np.arange(v.shape[0]),v] = 1\n",
    "    return oh\n",
    "\n",
    "def cp_samples_to_class_prob(cp_, X):\n",
    "    # print(cp_.shape, X.shape)\n",
    "    if type(X) is not np.ndarray:\n",
    "        X = X.numpy()\n",
    "    # cl_ = np.zeros((X.shape[0],cp_.shape[0]))\n",
    "    one_hot_samples = np.zeros((cp_.shape[0],X.shape[0],cp_.shape[1]+1))\n",
    "    # print(one_hot_samples.shape)\n",
    "    for i in range(cp_.shape[0]):\n",
    "        cl = change_points_to_labels(cp_[i,:], X)\n",
    "        # print(cp_[i,:], np.unique(cl))\n",
    "        one_hot_samples[i,:,:] = one_hot_np(cl, cp_[i,:], X)\n",
    "    probs = np.mean(one_hot_samples, axis = 0)\n",
    "    return probs\n",
    "\n",
    "def model_1D_joint_change_points_w_bounds_Bayesian(data):\n",
    "    # print('starting')\n",
    "    noise=torch.tensor(0.01)\n",
    "    jitter=torch.tensor(1.0e-5)\n",
    "    \n",
    "    xs = to_torch(data[0])\n",
    "    ys = to_torch(data[1])\n",
    "    xf = to_torch(data[2])\n",
    "    yf = to_torch(data[3])\n",
    "    Xpred = to_torch(data[4])\n",
    "    num_regions = to_torch(data[5])\n",
    " \n",
    "    if len(yf.shape) == 1:\n",
    "        yf = yf[:,None]\n",
    "        \n",
    "    uL, _ = torch.sort(ys)\n",
    "    if uL.shape[0] == 2:\n",
    "        num_regions = 2\n",
    "        for i in range(2):\n",
    "            ys[ys == uL[i]] = i\n",
    "    \n",
    "    Xsf = torch.vstack((xs,xf))\n",
    "    idx_st = torch.arange(xs.shape[0])\n",
    "    idx_fp = torch.arange(xf.shape[0]) + xs.shape[0]\n",
    "    # print('before sampling')\n",
    "    \n",
    "    changepoint_bounds_min, changepoint_bounds_max = bounds_set_csp(num_regions, xs, ys)\n",
    "    # print('bounds', changepoint_bounds_min, changepoint_bounds_max)\n",
    "    gp_var_bound_min = 1.*torch.ones((num_regions,yf.shape[1])).double()\n",
    "    gp_var_bound_max = 20.*torch.ones((num_regions,yf.shape[1])).double()\n",
    "    gp_lengthscale_bound_min = 1.*torch.ones((num_regions,yf.shape[1])).double()\n",
    "    gp_lengthscale_bound_max = 20.*torch.ones((num_regions,yf.shape[1])).double()\n",
    "    \n",
    "    changepoint_ = pyro.sample('changepoint_', dist.Uniform(changepoint_bounds_min.flatten(),changepoint_bounds_max.flatten())).double()\n",
    "    gp_noise = pyro.sample(\"gp_noise\", dist.Uniform(0.01, .1)).double()\n",
    "    gp_var = pyro.sample(\"gp_var\", dist.Uniform(gp_var_bound_min, gp_var_bound_max)).double()\n",
    "    gp_lengthscale = pyro.sample(\"gp_lengthscale\", dist.Uniform(gp_lengthscale_bound_min, gp_lengthscale_bound_max)).double()\n",
    "    \n",
    "    cluster_labels = change_points_to_labels_torch(changepoint_, Xsf)\n",
    "    membership = one_hot(cluster_labels.long()).double()\n",
    "    \n",
    "    region_labels = change_points_to_labels_torch(changepoint_, Xsf)\n",
    "    membership = one_hot(region_labels, num_regions)\n",
    "\n",
    "    mean_full = torch.zeros((xf.shape[0],yf.shape[1])).double()\n",
    "    kernel_full = torch.zeros((xf.shape[0],xf.shape[0],yf.shape[1])).double()\n",
    "    prior_means = yf.flatten().mean()*torch.ones((num_regions, yf.shape[1])).double() # THIS SHOULD BE UPDATED!!!\n",
    "    \n",
    "    yf_mean_removed = torch.clone(yf).double()\n",
    "    \n",
    "    for i in range(num_regions):\n",
    "        idx = torch.argwhere(region_labels[idx_fp] == i).flatten() # just fp data points\n",
    "\n",
    "        if idx.numel() > 0:\n",
    "            for j in range(yf.shape[1]):\n",
    "                xx = xf[idx,:].double()\n",
    "                yy = yf[idx,j][:,None].double() - yf[idx,j].mean().double()\n",
    "                \n",
    "                prior_means[i,j] = yf[idx,j].mean().double()\n",
    "\n",
    "                mean_xx_post, K_xx_post = gp_forward(xx, xx, yy, gp_var[i,j], gp_lengthscale[i,j], gp_noise)\n",
    "                mean_full[idx,j]=mean_xx_post.flatten().double()\n",
    "                kernel_full[idx.flatten()[:,None],idx.flatten(),j]=K_xx_post.double()\n",
    "\n",
    "                yf_mean_removed[idx,j] = yf[idx,j].double() - yf[idx,j].mean().double()\n",
    "    \n",
    "    pyro.deterministic('prior_means', prior_means)\n",
    "    # print('kernel_full:', kernel_full.squeeze())\n",
    "    \n",
    "    for j in range(yf.shape[1]):\n",
    "        kernel_full[:,:,j] += jitter * torch.eye(xf.shape[0]).double()\n",
    "    \n",
    "    # print(changepoint_, mean_full.flatten(), kernel_full.squeeze(), yf_mean_removed.flatten())\n",
    "    pyro.sample(\"obs\", dist.MultivariateNormal(loc=mean_full.flatten(), covariance_matrix=kernel_full.squeeze()), obs=yf_mean_removed.flatten())\n",
    "\n",
    "def gpr_piecewise_forward_w_mean(Xpred, Xtrain, Ytrain, cp, var, lengthscale, noise, prior_for_means):\n",
    "    # print('prior_means:', prior_means)\n",
    "    Xtrain = to_torch(Xtrain)\n",
    "    Ytrain = to_torch(Ytrain)\n",
    "    Xpred = to_torch(Xpred)\n",
    "    cp = to_torch(cp)\n",
    "    var = to_torch(var)\n",
    "    lengthscale = to_torch(lengthscale)\n",
    "    noise = to_torch(noise)\n",
    "    prior_for_means = to_torch(prior_for_means.flatten())\n",
    "    \n",
    "    cp, _ = torch.sort(to_torch(cp.flatten()))\n",
    "    cp = cp.flatten()\n",
    "    \n",
    "    # predict segment before first changepoint.\n",
    "    idx_pred = Xpred[:,0] < cp[0]\n",
    "    idx_train = Xtrain[:,0] < cp[0]\n",
    "    m0, c0 = gp_forward(Xpred[idx_pred,:], Xtrain[idx_train,:], Ytrain[idx_train,:]-prior_for_means[0], var[0], lengthscale[0], noise)\n",
    "    mean = [m0+prior_for_means[0]]\n",
    "    cov = [c0]\n",
    "    xsegment = [Xpred[idx_pred,:]]\n",
    "    idxsegment = [idx_pred]\n",
    "    \n",
    "    # predict all other segments.\n",
    "    for i in range(cp.shape[0]):\n",
    "        if i == cp.shape[0]-1:\n",
    "            idx_pred = Xpred[:,0] > cp[i]\n",
    "            idx_train = Xtrain[:,0] > cp[i]\n",
    "        else:\n",
    "            idx_pred = torch.logical_and(Xpred[:,0] > cp[i], Xpred[:,0] < cp[i+1])\n",
    "            idx_train = torch.logical_and(Xtrain[:,0] > cp[i], Xtrain[:,0] < cp[i+1])\n",
    "            \n",
    "        m_curr, c_curr = gp_forward(Xpred[idx_pred,:], Xtrain[idx_train,:], Ytrain[idx_train,:]-prior_for_means[i+1], var[i+1], lengthscale[i+1], noise)\n",
    "        mean.append(m_curr+prior_for_means[i+1])\n",
    "        cov.append(c_curr)\n",
    "        xsegment.append(Xpred[idx_pred,:])\n",
    "        idxsegment.append(idx_pred)\n",
    "    \n",
    "    return mean, cov, xsegment, idxsegment\n",
    "\n",
    "def gp_forward(Xtest, Xtrain, Ytrain, var, lengthscale, noise):\n",
    "    # Derived from: https://num.pyro.ai/en/0.7.1/examples/gp.html\n",
    "    k_pp = gpkernel(Xtest, Xtest, var, lengthscale, noise, include_noise=True)\n",
    "    k_pX = gpkernel(Xtest, Xtrain, var, lengthscale, noise, include_noise=False)\n",
    "    k_XX = gpkernel(Xtrain, Xtrain, var, lengthscale, noise, include_noise=True)\n",
    "\n",
    "    #k_XX = torch.tensor( nearestPD(k_XX.detach().numpy()) ).double()\n",
    "    K_xx_inv = torch.linalg.inv(k_XX.double()).double()\n",
    "    # print(k_XX.shape, K_xx_inv.shape, Xtest.shape, Xtrain.shape, Ytrain.shape)\n",
    "    K_xx_post = k_pp.double() - torch.matmul(k_pX.double(), torch.matmul(K_xx_inv.double(), k_pX.T.double())).double()\n",
    "    mean_xx_post = torch.matmul(k_pX.double(), torch.matmul(K_xx_inv.double(), Ytrain.double()))\n",
    "    return mean_xx_post, K_xx_post\n",
    "\n",
    "def gpkernel(X, Z, var, length, noise, jitter=1.0e-6, include_noise=True):\n",
    "    deltaXsq = torch.pow((X.double() - Z.T.double()) / length.double(), 2.0)\n",
    "    k = var.double() * torch.exp(-0.5 * deltaXsq).double()\n",
    "    if include_noise:\n",
    "        k += (noise.double() + jitter) * torch.eye(X.shape[0]).double()\n",
    "    return k\n",
    "\n",
    "def bounds_set_csp(num_regions, xs, ys):\n",
    "    xs = xs.flatten()\n",
    "    ys = ys.flatten()\n",
    "    bounds_min = torch.ones((num_regions-1,1))*xs.min()\n",
    "    bounds_max = torch.ones((num_regions-1,1))*xs.max()\n",
    "    uL = torch.unique(ys)\n",
    "    if uL.shape[0] == 2:\n",
    "        bounds_min = torch.ones((num_regions-1,1))*xs[ys == 0].max()\n",
    "        idx_y_is_1_or_2 = torch.logical_or(ys == 1, ys == 2)\n",
    "        bounds_max = torch.ones((num_regions-1,1))*xs[idx_y_is_1_or_2].min()\n",
    "    if uL.shape[0] > 2:\n",
    "        bounds_points = torch.zeros((uL.shape[0],2))\n",
    "        bounds_min = []\n",
    "        bounds_max = []\n",
    "        for i in range(uL.shape[0]):\n",
    "            bounds_points[i,:] = torch.tensor([xs[ys == uL[i]].min(), xs[ys == uL[i]].max()])\n",
    "        sorted_bounds, _ = torch.sort(bounds_points.flatten())\n",
    "        \n",
    "        for i in range(1, sorted_bounds.shape[0]-2, 2):\n",
    "            bounds_min.append(sorted_bounds[i])\n",
    "            bounds_max.append(sorted_bounds[i+1])\n",
    "\n",
    "    bounds_min = torch.tensor(bounds_min)\n",
    "    bounds_max = torch.tensor(bounds_max)\n",
    "    return bounds_min, bounds_max\n",
    "\n",
    "def change_points_to_labels_torch(cp, X):\n",
    "    if type(X) is np.ndarray:\n",
    "        X = torch.tensor(X)\n",
    "    cp,_ = torch.sort(cp)\n",
    "    cl = torch.zeros((X.shape[0])).long()\n",
    "    N = cp.shape[0] # N = 3\n",
    "    for i in range(0, N):\n",
    "        if i < N-1:\n",
    "            idx = torch.logical_and(X > cp[i], X < cp[i+1])\n",
    "        elif i == N-1:\n",
    "            idx = X > cp[i]\n",
    "        cl[idx.flatten()] = i+1\n",
    "    return cl\n",
    "\n",
    "def model_pm_1D_CP_w_bounds(data):\n",
    "    num_regions = 3\n",
    "    noise=torch.tensor(0.01)\n",
    "    jitter=torch.tensor(1.0e-5)\n",
    "    \n",
    "    xs = to_torch(data[0])\n",
    "    ys = to_torch(data[1])\n",
    "        \n",
    "    uL, _ = torch.sort(ys)\n",
    "    if uL.shape[0] == 2:\n",
    "        num_regions = 2\n",
    "        for i in range(2):\n",
    "            ys[ys == uL[i]] = i\n",
    "    \n",
    "    changepoint_bounds_min, changepoint_bounds_max = bounds_set_csp(num_regions, xs, ys)\n",
    "    changepoint_ = pyro.sample('changepoint_', dist.Uniform(changepoint_bounds_min.flatten(),changepoint_bounds_max.flatten())).double()\n",
    "\n",
    "    region_labels = change_points_to_labels_torch(changepoint_, xs)\n",
    "    membership = one_hot(region_labels, num_regions)\n",
    "\n",
    "    # print('look here:', xs.flatten(), ys.flatten(), changepoint_.flatten())\n",
    "    pyro.sample('obs', dist.Categorical(logits=membership), obs=ys.flatten().double())   \n",
    "\n",
    "def normalize(v):\n",
    "    v = np.ma.array(v, mask=np.isnan(v))\n",
    "    if len(v.shape) == 1:\n",
    "        nv = (v - v.min())/(v.max()-v.min())\n",
    "    else:\n",
    "        nv = v.copy()\n",
    "        for i in range(v.shape[0]):\n",
    "            nv[i,:] = (v[i,:] - v[i,:].min())/(v[i,:].max()-v[i,:].min())\n",
    "    return nv\n",
    "\n",
    "def cosd(deg):\n",
    "    # cosine with argument in degrees\n",
    "    return np.cos(deg * np.pi/180)\n",
    "\n",
    "def sind(deg):\n",
    "    # sine with argument in degrees\n",
    "    return np.sin(deg * np.pi/180)\n",
    "\n",
    "def tern2cart(T):\n",
    "    # convert ternary data to cartesian coordinates\n",
    "    sT = np.sum(T,axis = 1)\n",
    "    T = 100 * T / np.tile(sT[:,None],(1,3))\n",
    "\n",
    "    C = np.zeros((T.shape[0],2))\n",
    "    C[:,1] = T[:,1]*sind(60)/100\n",
    "    C[:,0] = T[:,0]/100 + C[:,1]*sind(30)/sind(60)\n",
    "    return C\n",
    "\n",
    "# Data Storage -------------------------\n",
    "def set_dynamic_params(reset_storage_variables = False, reset_rep_variables = False, store_results = False):\n",
    "    # setting up variables for storing data\n",
    "    global params_dynamic\n",
    "    if reset_rep_variables:\n",
    "        params_dynamic['BO_iter'] = 1\n",
    "        params_dynamic['iter'] = 1\n",
    "        params_dynamic['phase_map_converged'] = False\n",
    "        params_dynamic['first_iter'] = True    \n",
    "    \n",
    "    if reset_storage_variables:\n",
    "        params_dynamic['cluster_results_by_rep'] = []\n",
    "        params_dynamic['measured_samples_XRD'] = []\n",
    "        params_dynamic['measured_samples_FP'] = []\n",
    "        params_dynamic['measured_samples_XRD_by_rep'] = []\n",
    "        params_dynamic['measured_samples_FP_by_rep'] = []\n",
    "        params_dynamic['measured_samples_XRD'] = []\n",
    "        params_dynamic['measured_samples_FP'] = []\n",
    "        params_dynamic.pop('cluster_results', None) # remove this field. use its absence to populate in 'store' function.\n",
    "\n",
    "    # this part happens at the end of a CAMEO rep.\n",
    "    if store_results:\n",
    "        params_dynamic['cluster_results_by_rep'].append(params_dynamic['cluster_results'])\n",
    "        params_dynamic.pop('cluster_results', None) # remove this field. use its absence to populate in 'store' function.\n",
    "\n",
    "        if params['method'][:5] == 'coreg':\n",
    "            params_dynamic['measured_samples_XRD_by_rep'].append(params_dynamic['measured_samples_XRD'])\n",
    "            params_dynamic['measured_samples_FP_by_rep'].append(params_dynamic['measured_samples_FP'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Pl9yYQbG5K-P"
   },
   "source": [
    "# Create Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:35:40.732145Z",
     "start_time": "2023-03-10T21:35:40.706146Z"
    },
    "code_folding": [
     37,
     45,
     72,
     83,
     86,
     127,
     155,
     163,
     173,
     185
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1655590801247,
     "user": {
      "displayName": "Aaron Gilad Kusne",
      "userId": "07808887398243303382"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "owkRnDZm5Lmo",
    "outputId": "cc19dacf-e062-4a30-c2ec-ee7b9e0fc2f2"
   },
   "outputs": [],
   "source": [
    "%%writefile instruments_230101a.py\n",
    "\n",
    "from infrastructure_230101a import match_rows_in_matrix, normalize_each_row_by_sum, tern2cart, similarity_matrix\n",
    "from global_variables_and_monitors_230101a import performance_monitor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "from collections import namedtuple\n",
    "import simpy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from scipy.spatial import distance as scipy_dist\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from scipy.special import gamma\n",
    "\n",
    "# create class for synthesis instruments\n",
    "# initialize instrument as a resource\n",
    "\n",
    "# inst_params = {'Measure_Time':Measure_Time, 'Synth_Time':Synth_Time, \n",
    "#                'save_dir':r'/content/gdrive/My Drive/Research/jupyter/Networked ML/gpyflow_model',\n",
    "#                'meas_type':meas_type,\n",
    "#                'sample_pool_manager':spm_i}\n",
    "\n",
    "class instrument_synthesis:\n",
    "    def __init__(self, env, params, unique_ID):\n",
    "        self.unique_ID = unique_ID\n",
    "        self.env = env\n",
    "        self.sample_synthesis = simpy.Resource(env, capacity = 1) # can only work on one job at a time.\n",
    "        self.Synth_Time = params['Synth_Time']\n",
    "        self.curr_synth_list = None\n",
    "        self.spm = params['sample_pool_manager']\n",
    "        \n",
    "    def synthesis(self, target_composition):\n",
    "        # function called to synthesize a sample at composition target_composition and add it to the sample_pool.\n",
    "        if type(target_composition) is np.ndarray:\n",
    "            target_composition = target_composition.item()\n",
    "        #\n",
    "        self.spm.synth_list_add(target_composition)\n",
    "        # system yields to environment for amount of time Synth_Time and then returns\n",
    "        # assume when it returns we have the sample\n",
    "        \n",
    "        yield self.env.timeout(self.Synth_Time)\n",
    "        #     print('finished synth')\n",
    "        max_item_index = self.spm.get_sample_index_max()\n",
    "        # print('synth, max item in sample pool:', max_item_index)\n",
    "        # add the new sample to the sample_pool.\n",
    "        new_sample_index = max_item_index\n",
    "        Sample = namedtuple('Sample', 'sample_index, composition, structure, funcprop, status')\n",
    "        self.spm.sample_pool_append(Sample(new_sample_index, target_composition, 0, 0, 'in'))\n",
    "        self.spm.synth_list_remove(target_composition)\n",
    "\n",
    "        print(f'synth {str(self.unique_ID)[:5]} synthesized sample c:{target_composition} at {self.env.now}.')\n",
    "        self.spm.all_synthesized.append(target_composition) # add new synthesized to list of all synthesized.\n",
    "        # print(f'put {target_composition} in pool')\n",
    "        return new_sample_index # return the index for the new sample.\n",
    "\n",
    "class instrument_measure:\n",
    "    # create class for measurement instruments\n",
    "    # set up each measurement instrument as a resource that can only work on one job at a time.\n",
    "    def __init__(self, env, focus, params, unique_ID):\n",
    "        self.focus = focus\n",
    "        self.unique_ID = unique_ID\n",
    "        self.env = env\n",
    "        self.sample_measure = simpy.Resource(env, capacity = 1) # can only work on one job at a time.\n",
    "        self.Measure_Time = params['Measure_Time']\n",
    "        self.meas_type = params['meas_type'] #'sim'\n",
    "        self.save_dir = params['save_dir']\n",
    "        self.spm = params['sample_pool_manager']\n",
    "        if self.meas_type == 'sim_Raman_real' or self.meas_type == 'sim_Raman_hard':\n",
    "            self.struct_data = np.load(self.save_dir + '/raman.npz')\n",
    "            if self.meas_type == 'sim_Raman_real':\n",
    "                self.func_prop = tf.saved_model.load(self.save_dir)\n",
    "            \n",
    "    def measure(self, target_composition):\n",
    "        # function called to request the measurement of sample index target_index in the sample_pool\n",
    "        # returns the measurement value.\n",
    "        if type(target_composition) is np.ndarray:\n",
    "            target_composition = target_composition.item()\n",
    "        \n",
    "        get0 = self.spm.list_sample_pool_items()\n",
    "        sample = yield self.env.process(self.spm.get_sample(target_composition))\n",
    "        get1 = self.spm.list_sample_pool_items()\n",
    "        \n",
    "        print(f'meas-{self.focus} {str(self.unique_ID)[:5]} got c:{sample.composition} at {self.env.now}')\n",
    "        \n",
    "        self.spm.lent_list_add(target_composition, self.focus)\n",
    "        \n",
    "        # if we're requesting a mag measurement, then timeout for Measure_Time and return the measurement.\n",
    "        if self.focus == 'funcprop': \n",
    "            yield self.env.timeout(self.Measure_Time)\n",
    "            measurement_results = self.measure_funcprop(sample.composition)\n",
    "        # if we're requesting an xrd measurement, then timeout for Measure_Time and return the measurement.\n",
    "        elif self.focus == 'structure':\n",
    "            # print(f'structure measuring {sample.sample_index}')\n",
    "            yield self.env.timeout(self.Measure_Time)\n",
    "            measurement_results = self.measure_structure(sample.composition)\n",
    "        \n",
    "        put0 = self.spm.list_sample_pool_items()\n",
    "        yield self.env.process(self.spm.put_sample(sample))\n",
    "        put1 = self.spm.list_sample_pool_items()\n",
    "        \n",
    "        sget0 = sample.composition in np.asarray([s.composition for s in get0])\n",
    "        sget1 = sample.composition in np.asarray([s.composition for s in get1])\n",
    "        sput0 = sample.composition in np.asarray([s.composition for s in put0])\n",
    "        sput1 = sample.composition in np.asarray([s.composition for s in put1])\n",
    "        \n",
    "        print(f'>>>{self.focus} {repr(self.unique_ID)[:5]} want c:{target_composition}, got c:{sample.composition} pre/post get:{sget0}/{sget1}, put:{sput0}/{sput1}')\n",
    "        \n",
    "        # !!!!!!!!!!!!! SHOULD THIS BE COMPOSITION BASED???\n",
    "        self.spm.lent_list_remove(target_composition)\n",
    "        print(f'meas-{self.focus} {str(self.unique_ID)[:5]} released c:{sample.composition} at {self.env.now}')\n",
    "        yield self.env.timeout(1)\n",
    "        return measurement_results # return the measurement\n",
    "   \n",
    "    def measure_funcprop(self, X):\n",
    "        # function for simulating measuring functional property\n",
    "        X = np.atleast_1d(X)\n",
    "        if self.meas_type == 'sim':\n",
    "            y = np.exp(-1*((X-0.05)**2)/(5E-3)) + .1*np.exp(-1*((X-1)**2)/(1)) + np.random.normal(0., .01, X.shape)\n",
    "        elif self.meas_type == 'sim_Raman_hard':\n",
    "            # split at 8 and 14\n",
    "            # Current paper figure for hard challenge\n",
    "            yg0 = .2*np.exp(-.5*(X-5.)**2 / 2.)\n",
    "            yg0[X > 8.] = 0\n",
    "            yg1 = .2*np.exp(-.5*(X-11.)**2 / 2.)\n",
    "            yg1[X < 8.] = 0\n",
    "            yg1[X > 14.1] = 0\n",
    "            yg2 = 2*np.exp(-.5*(X-14.1)**2 / 1.)\n",
    "            yg2[X < 14.1] = 0\n",
    "            y = yg0 + yg1 + yg2\n",
    "            y = 24.*y+60.\n",
    "            \n",
    "        elif self.meas_type == 'sim_Raman_real':\n",
    "            X = X[:,None]\n",
    "            y, y_var = self.func_prop.predict_f_compiled(X)\n",
    "            y = y.numpy()\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def gamma_dist(self, x, a, beta):\n",
    "        return beta**a * x**(a-1) * np.exp(-beta*x) / gamma(a)\n",
    "\n",
    "    def measure_structure(self, X):\n",
    "        # function for simulating measuring structure\n",
    "        X = np.atleast_1d(X)\n",
    "        if self.meas_type == 'sim':\n",
    "            return self.struct_sim(X)\n",
    "        elif self.meas_type == 'sim_Raman_real' or self.meas_type == 'sim_Raman_hard':\n",
    "            return self.sim_Raman(X)\n",
    "    \n",
    "    def sim_Raman(self, X):\n",
    "        X = np.atleast_1d(X)\n",
    "        y = []\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            y_temp = self.gen_Raman(x)\n",
    "            y.append(y_temp.flatten())\n",
    "        y = np.asarray(y)\n",
    "        return y\n",
    "            \n",
    "    def gen_Raman(self, x):\n",
    "        m = self.struct_data['mn']\n",
    "        u = self.struct_data['vr']\n",
    "        if x <= 8.:\n",
    "            y  = np.random.multivariate_normal(m[0,:].squeeze(), u[0,:,:].squeeze(), 1).T\n",
    "        elif x > 14.1:\n",
    "            y  = np.random.multivariate_normal(m[2,:].squeeze(), u[2,:,:].squeeze(), 1).T\n",
    "        else:\n",
    "            y  = np.random.multivariate_normal(m[1,:].squeeze(), u[1,:,:].squeeze(), 1).T\n",
    "        yy = y + np.random.normal(0, np.sqrt(1E-6), y.shape)\n",
    "        return yy[::30]\n",
    "    \n",
    "    def struct_sim(self, X):\n",
    "        t = np.arange(0,1,.1)\n",
    "        structure = np.zeros((X.shape[0], t.shape[0]))\n",
    "        for idx, x in enumerate(X):\n",
    "            if x <= .25:\n",
    "                structure[idx,:] = 5.*np.exp(-1*((t-0.25)**2)/(1E-2)) + np.random.normal(0., .01, t.shape)\n",
    "            else:\n",
    "                structure[idx,:] = 5.*np.exp(-1*((t-0.75)**2)/(1E-2)) + np.random.normal(0., .01, t.shape)\n",
    "        return np.round_(structure, 2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "qCvVbciD5iFS"
   },
   "source": [
    "# Set up global module with performance monitor and global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:35:43.967111Z",
     "start_time": "2023-03-10T21:35:43.945145Z"
    },
    "code_folding": [
     28,
     91,
     93,
     100,
     112,
     123,
     185,
     186,
     190,
     201,
     204,
     207,
     209,
     211,
     237,
     243,
     277,
     313,
     329,
     338
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1655590803031,
     "user": {
      "displayName": "Aaron Gilad Kusne",
      "userId": "07808887398243303382"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "TSm9ZGhS5kBv",
    "outputId": "25af3beb-68b0-4150-8b61-81e175e06d29"
   },
   "outputs": [],
   "source": [
    "%%writefile global_variables_and_monitors_230101a.py\n",
    "\n",
    "from infrastructure_230101a import match_rows_in_matrix\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "from collections import namedtuple\n",
    "import simpy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from scipy.spatial import distance as scipy_dist\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from IPython.display import display\n",
    "\n",
    "class sample_pool_manager:\n",
    "    def __init__(self, env, name):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.sample_index_max = 0\n",
    "        self.sample_pool = simpy.FilterStore(env)\n",
    "        self.lent_list = []\n",
    "        self.lent_purpose = []\n",
    "        self.synth_list = []\n",
    "        self.status = []\n",
    "        self.all_synthesized = [2., 17.]\n",
    "    def get_sample_index_max(self):\n",
    "        return self.sample_index_max\n",
    "    def increment_sample_index_max(self):\n",
    "        self.sample_index_max = self.sample_index_max + 1\n",
    "        return self.sample_index_max\n",
    "    def list_sample_pool_items(self):\n",
    "        return self.sample_pool.items.copy()\n",
    "    def lent_list_add(self, comp, meas_method):\n",
    "        # print('lent sample indices', lent_sample_indices)\n",
    "        self.lent_list.append(comp)\n",
    "        self.lent_purpose.append(meas_method)   \n",
    "    def lent_list_remove(self, comp):\n",
    "        # print('inside put', sample_index, self.lent_list)\n",
    "        idx = np.where(self.lent_list == comp)[0]\n",
    "        if idx.flatten().shape[0] > 0:\n",
    "            del(self.lent_list[idx[0].item()])\n",
    "            del(self.lent_purpose[idx[0].item()])\n",
    "    def sample_pool_append(self, items):\n",
    "        if isinstance(items, list):\n",
    "            for item in items:\n",
    "                self.sample_pool.items.append(item)\n",
    "                self.sample_index_max = self.sample_index_max + 1\n",
    "        else:\n",
    "            self.sample_pool.items.append(items)\n",
    "            self.sample_index_max = self.sample_index_max + 1\n",
    "    def synth_list_add(self,comp):\n",
    "        if type(comp) is np.ndarray:\n",
    "            comp = comp.item()\n",
    "        self.synth_list.append(comp)\n",
    "    def synth_list_remove(self,comp):\n",
    "        drop_idx = [idx for idx,c in enumerate(self.synth_list) if c == comp]\n",
    "        if drop_idx:\n",
    "            del(self.synth_list[drop_idx[0]])\n",
    "    def get_sample(self, composition):\n",
    "        condition = lambda sample: sample.composition == composition\n",
    "        cont_wait = True\n",
    "        while cont_wait:\n",
    "            items = self.list_sample_pool_items()\n",
    "            compositions = np.asarray([item.composition for item in items])\n",
    "            # print('compositions in spm:', compositions.flatten())\n",
    "            if composition in compositions:\n",
    "                sample = yield self.sample_pool.get(condition)\n",
    "                cont_wait = False\n",
    "            else:\n",
    "                print(f'waiting for c:{composition}')\n",
    "                yield self.env.timeout(1)\n",
    "        return sample   \n",
    "    def put_sample(self, sample):\n",
    "        self.sample_pool.items.append(sample)\n",
    "        yield self.env.timeout(1)\n",
    "\n",
    "class performance_monitor:\n",
    "    # class for monitoring active learning phase mapping and bayesian materials optimization performance\n",
    "    def history_tracking(history, new_record):\n",
    "        if history is None:\n",
    "            history = new_record\n",
    "        else:\n",
    "            # print(f'history concat {history.shape}, {new_record.shape}')\n",
    "            history = np.concatenate((history,new_record),axis=0)\n",
    "        return history\n",
    "    def phase_mapping_performance(phase_map_estimate, x_range):\n",
    "        # phase mapping performance using FMI measure\n",
    "        # assume phase_map_estimate is a matrix of row vectors\n",
    "        xx = np.arange(x_range[0],x_range[1],x_range[2]).flatten()[None,:]\n",
    "        LABELS_TRUE = np.zeros(xx.shape)\n",
    "        LABELS_TRUE[xx > 8] = 1\n",
    "        LABELS_TRUE[xx > 14.1] = 2\n",
    "        measure_fmi = np.zeros((phase_map_estimate.shape[0],1))\n",
    "        for i in range(phase_map_estimate.shape[0]):\n",
    "            # print('inside fmi for loop:', LABELS_TRUE.shape, phase_map_estimate[i,:].shape )\n",
    "            measure_fmi[i] = fmi(LABELS_TRUE.squeeze(), phase_map_estimate[i,:].squeeze())\n",
    "        return measure_fmi\n",
    "    def simple_regret(val_by_iter, meas_type):\n",
    "        # simple regret performance measure\n",
    "        if meas_type == 'sim_Raman_real':\n",
    "            TRUE_MAX = 109.92\n",
    "        elif meas_type == 'sim_Raman_hard':\n",
    "            TRUE_MAX = 110.23\n",
    "        measure_regret = np.zeros(val_by_iter.shape)\n",
    "        max_by_iter = np.maximum.accumulate(val_by_iter)\n",
    "        simple_regret = TRUE_MAX - max_by_iter\n",
    "        return simple_regret/TRUE_MAX\n",
    "\n",
    "class agt_repository:\n",
    "    def __init__(self):\n",
    "        self.agt_repo = None   \n",
    "    def init_agt_repo(self, AI_list):\n",
    "        ID_list = np.asarray( [entry.unique_ID for entry in AI_list] ).squeeze()\n",
    "        AI_type = [entry.focus for entry in AI_list]\n",
    "        agt_focus_list = [entry.focus for entry in AI_list]\n",
    "        n_ = [None for i in range(ID_list.shape[0])]\n",
    "        data = {'unique_ID': ID_list, 'AI_type': AI_type, 'indep_var': n_, 'curr_acq': n_, 'curr_pred': n_}\n",
    "        data_repository = pd.DataFrame(data = data) # hand off info to the central representation.\n",
    "        self.agt_repo = data_repository\n",
    "    def update_record(self, unique_ID, indep_var, curr_acq, curr_pred):\n",
    "        # Assumes inputs are vectors and converts to single entry list.\n",
    "        # print('update_record: curr_acq:', curr_acq[:5])\n",
    "        # print('update_record: norm curr_acq:', normalize(curr_acq.flatten())[:5])\n",
    "        df_index_to_update = self.agt_repo[self.agt_repo['unique_ID']==unique_ID].index[0] # find the sample to update with measurements.\n",
    "        df_index_to_update = np.atleast_1d(df_index_to_update)\n",
    "        self.agt_repo['indep_var'].iloc[df_index_to_update] = [indep_var] # update value\n",
    "        self.agt_repo['curr_acq'].iloc[df_index_to_update] = [normalize(curr_acq.flatten())] # update value\n",
    "        self.agt_repo['curr_pred'].iloc[df_index_to_update] = [curr_pred] # update value\n",
    "    def get_other_records(self, unique_ID):\n",
    "        df_indices_to_grab = self.agt_repo[self.agt_repo['unique_ID'] != unique_ID].index.to_numpy()\n",
    "        # print('indices:', df_indices_to_grab)\n",
    "        df_indices_to_grab = np.atleast_1d(df_indices_to_grab)\n",
    "        indep_var = self.agt_repo['indep_var'].iloc[df_indices_to_grab[0]]\n",
    "        acq_mean, acq_all = self.series_mean('curr_acq', df_indices_to_grab)\n",
    "        mean_fp, data_fp, mean_st, data_st = self.type_mean('curr_acq', df_indices_to_grab)\n",
    "        # pred_mean, pred_all = self.series_mean('curr_pred', df_indices_to_grab)\n",
    "        data_mean = {'indep_var': indep_var, 'acq_mean': acq_mean, 'pred_mean': 'Not used', \\\n",
    "                     'mean_fp':mean_fp,'data_fp':data_fp,'mean_st':mean_st,'data_st':data_st}\n",
    "        data_all = {'indep_var': indep_var, 'acq_all': acq_all, 'pred_all': 'Not used'}\n",
    "        return data_mean, data_all\n",
    "    def get_repo(self):\n",
    "        return self.agt_repo\n",
    "    def series_mean(self, col, idx):\n",
    "        temp = self.agt_repo[col].iloc[idx].to_numpy()\n",
    "        data = [entry.flatten() for entry in temp if entry is not None]\n",
    "        series_mean = None\n",
    "        if data:\n",
    "            data = np.vstack( data )\n",
    "            data = np.ma.array(data, mask=np.isnan(data))\n",
    "            series_mean = data.mean(axis = 0).flatten()\n",
    "        return series_mean, data\n",
    "    def type_mean(self, col, idx):\n",
    "        temp = self.agt_repo[col].iloc[idx].to_numpy()\n",
    "        AI_type = self.agt_repo['AI_type'].iloc[idx].to_numpy() # 'funcprop', 'structure'\n",
    "        \n",
    "        data_fp = [entry.flatten() for count, entry in enumerate(temp) if entry is not None and AI_type[count]=='funcprop']\n",
    "        data_st = [entry.flatten() for count, entry in enumerate(temp) if entry is not None and AI_type[count]=='structure']\n",
    "        \n",
    "        mean_fp = None\n",
    "        mean_st = None\n",
    "        if data_fp:\n",
    "            data_fp = np.vstack( data_fp )\n",
    "            data_fp = np.ma.array(data_fp, mask=np.isnan(data_fp))\n",
    "            mean_fp = data_fp.mean(axis = 0).flatten()\n",
    "        if data_st:\n",
    "            data_st = np.vstack( data_st )\n",
    "            data_st = np.ma.array(data_st, mask=np.isnan(data_st))\n",
    "            mean_st = data_st.mean(axis = 0).flatten()\n",
    "        return mean_fp, data_fp, mean_st, data_st\n",
    "    \n",
    "class mat_repository:\n",
    "    def __init__(self, env, spm):\n",
    "        self.spm = spm\n",
    "        self.env = env\n",
    "        self.mat_repo = self.init_mat_repo()\n",
    "    def init_mat_repo(self):\n",
    "        # get the compositions in the sample pool.\n",
    "        items = self.spm.list_sample_pool_items()\n",
    "        compositions = np.asarray([s.composition for s in items])\n",
    "        compositions = np.stack(compositions, axis = 0)\n",
    "        # initialize msir entries for each sample in sample pool.\n",
    "        n_ = [None for i in range(compositions.shape[0])]\n",
    "        data = {'sample_index': np.arange(compositions.shape[0]),'composition': compositions, 'structure': n_, 'funcprop': n_}\n",
    "        mat_repo = pd.DataFrame(data = data) # hand off info to the central representation.\n",
    "        # display(mat_repo)\n",
    "        return mat_repo        \n",
    "    def get_mat_data_all(self):\n",
    "        db = self.mat_repo\n",
    "        return db\n",
    "    def get_mat_data(self, data_type):\n",
    "        data = self._get_mat_data(data_type)\n",
    "        return data\n",
    "    def get_compositions(self):\n",
    "        return self.mat_repo['composition'].to_numpy()\n",
    "    def get_compositions_and_sample_indices(self):\n",
    "        return self.mat_repo['composition'].to_numpy(), self.mat_repo['sample_index'].to_numpy()\n",
    "    def _get_mat_data(self, data_type):\n",
    "        repo = self.mat_repo.copy()\n",
    "        # print('data_type to get:', data_type)\n",
    "        # display(self.mat_repo)\n",
    "        idx = [i for i in range(repo.shape[0]) if (repo[data_type].iloc[i]) is not None ]\n",
    "        # print('gotten mat_repo idx:', idx)\n",
    "        temp_y = repo[data_type].iloc[idx].to_numpy()\n",
    "        for i in range(len(temp_y)):\n",
    "            if type(temp_y[i]) is np.ndarray:\n",
    "                temp_y[i] = temp_y[i].flatten()\n",
    "            elif isinstance(temp_y[i], list):\n",
    "                temp_y[i] = np.asarray(temp_y[i]).flatten()\n",
    "        # print('gotten mat_repo data:', temp_y)\n",
    "        try:\n",
    "            y = np.stack( temp_y, axis=0 ).squeeze()\n",
    "        except:\n",
    "            print('FAILED TEMP_Y:')\n",
    "            for i in range(len(temp_y)):\n",
    "                print(temp_y[i])\n",
    "        temp_x = repo['composition'].iloc[idx].to_numpy()\n",
    "        x = np.stack( temp_x, axis = 0 )\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[:,None]\n",
    "        if len(y.shape) == 1:\n",
    "            y = y[:,None]\n",
    "        return (x, y)\n",
    "    def repositories_unify(self, repositories):\n",
    "        repo0 = respositories[0]\n",
    "        for idx, repo in enumerate(repositories):\n",
    "            if idx > 0:\n",
    "                repo0 = df_unify(repo0, repositories[idx])\n",
    "        return repo0\n",
    "    def update_from_sample_pool_wait_for_target(self, env, agent_ID, target_index, target_composition):\n",
    "        not_found = not np.sum(self.mat_repo['sample_index'].to_numpy() == target_index)\n",
    "        while not_found:\n",
    "            self.update_from_sample_pool()\n",
    "            \n",
    "#             lent = self.spm.lent_list\n",
    "            curr = self.spm.list_sample_pool_items()\n",
    "            curri = np.asarray([s.sample_index for s in curr])\n",
    "            currc = [s.composition for s in curr if s.sample_index == target_index]\n",
    "            currc_i = [s.sample_index for s in curr if s.composition == target_composition]\n",
    "            \n",
    "            print(f'{agent_ID} waiting for:{target_index} c:{target_composition}, spm:{currc_i},{target_composition};{target_index},{currc}')\n",
    "            # print('check if composition is in mat repo:')\n",
    "            # display(self.mat_repo)\n",
    "            # print('while loop condition:', np.sum(self.mat_repo['sample_index'].to_numpy() == target_index))\n",
    "            # print('while alt loop condition:', np.sum(self.mat_repo['composition'].to_numpy() == target_composition))\n",
    "            self.update_from_sample_pool()\n",
    "            \n",
    "            # if a sample with the right composition already exists in the mat_repo, use that one.\n",
    "            repo_idx = np.argwhere(self.mat_repo['composition'].to_numpy() == target_composition)\n",
    "            if repo_idx: \n",
    "                rpo_sidx = self.mat_repo['sample_index'].to_numpy()\n",
    "                target_index = rpo_sidx[repo_idx].item()\n",
    "                print('found in mat_repo, sample_index:', target_index)\n",
    "                # print(f'returned target_index:{target_index}')\n",
    "                not_found = False\n",
    "            # if a different target_index has the right composition in spm, use that one.\n",
    "            elif currc_i:\n",
    "                target_index = currc_i[0]\n",
    "                print(f'returned target_index:{target_index}')\n",
    "                not_found = False\n",
    "            else:\n",
    "                yield env.timeout(1)\n",
    "        return target_index   \n",
    "    def update_with_measurement(self, meas_type, target_composition, measurement_result):\n",
    "        # print('update_with_measurement:', meas_type, target_composition, measurement_result)\n",
    "        # for sample with index target_index, add its measurement value measurement_result\n",
    "        #print(type(measurement_result), measurement_result)\n",
    "        if np.max(measurement_result.shape) <= 1: # if the measurement is a scalar, turn it into a 1x1 matrix\n",
    "            measurement_result = np.asscalar(measurement_result)\n",
    "        elif type(measurement_result) is np.ndarray and measurement_result.flatten().shape[0] > 1:\n",
    "            measurement_result = [measurement_result]\n",
    "        \n",
    "        self.update_from_sample_pool()\n",
    "        compositions_ = self.mat_repo['composition'].to_numpy()\n",
    "        if np.any(compositions_ == target_composition):\n",
    "            df_index_to_update = np.where(compositions_ == target_composition)[0].flatten().astype(int)\n",
    "            if df_index_to_update.shape[0] > 1:\n",
    "                df_index_to_update = df_index_to_update[0]\n",
    "            # print('cmp dataframe index:',df_index_to_update)\n",
    "            self.mat_repo[meas_type].iloc[df_index_to_update] = measurement_result # update measurement value\n",
    "            self.mat_repo = self.mat_repo.sort_values(by=['sample_index']) # sort msir by sample_index\n",
    "            redo = False\n",
    "        else:\n",
    "            spm_items = self.spm.list_sample_pool_items()\n",
    "            c_lent = np.asarray(self.spm.lent_list)\n",
    "            c_spm = np.asarray([s.composition for s in spm_items]).flatten()\n",
    "            print(f'>>>> {meas_type} waiting for c:{target_composition}, repo:{compositions_.flatten()}, spm:{c_spm}, lent:{c_lent}')\n",
    "            print(f'>>>> All synthesized:{self.spm.all_synthesized}')\n",
    "            print(f'forcing synth of {target_composition}')\n",
    "            \n",
    "            max_item_index = self.spm.get_sample_index_max()\n",
    "            new_sample_index = max_item_index\n",
    "            Sample = namedtuple('Sample', 'sample_index, composition, structure, funcprop, status')\n",
    "            self.spm.sample_pool_append(Sample(new_sample_index, target_composition, 0, 0, ''))\n",
    "            yield self.env.timeout(1)\n",
    "            self.update_from_sample_pool()\n",
    "            redo = True\n",
    "        yield self.env.timeout(1)\n",
    "        return redo\n",
    "    def update_from_sample_pool(self):\n",
    "        # update cr with samples in sample_pool\n",
    "        store_items = self.spm.list_sample_pool_items()\n",
    "        compositions_pool = np.asarray([s.composition for s in store_items]).flatten()[:,None]\n",
    "        temp = self.mat_repo['composition'].to_numpy()\n",
    "        compositions_repo = np.stack(temp, axis=0 )\n",
    "        \n",
    "        if len(compositions_repo.shape) == 1: # if only one entry in the sample_pool, add an extra empty dimension\n",
    "            compositions_repo = compositions_repo[:,None]\n",
    "            \n",
    "        # for each sample in sample_pool, if there is no sample in msir that is within 1E-3 composition, then add it.    \n",
    "        for i in range(compositions_pool.shape[0]): \n",
    "            v = compositions_repo - np.tile(compositions_pool[i,:][None,:],(compositions_repo.shape[0],1))\n",
    "            d = np.min( np.linalg.norm(v, axis = 1) )\n",
    "            if d >= 1E-3:\n",
    "                self.add_mat_sample(store_items[i])      \n",
    "    def add_mat_sample(self, sample):\n",
    "        # add new sample to msir\n",
    "        # takes in a sample and adds its info to self.msir\n",
    "        temp = pd.DataFrame(data = {'sample_index': [sample.sample_index],'composition': sample.composition, 'structure': [None], 'funcprop': [None]} )\n",
    "        self.mat_repo = self.mat_repo.append(temp, ignore_index=True) \n",
    "        # display(self.mat_repo)\n",
    "        # print( tabulate(self.mat_repo, headers='keys', ) )\n",
    "        ''\n",
    "    \n",
    "def normalize(v):\n",
    "    if len(v.shape) == 1:\n",
    "        nv = (v - v.min())/(v.max()-v.min())\n",
    "    else:\n",
    "        nv = v.copy()\n",
    "        for i in range(v.shape[0]):\n",
    "            nv[i,:] = (v[i,:] - v[i,:].min())/(v[i,:].max()-v[i,:].min())\n",
    "    return nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoRb-IBJ5Z-h"
   },
   "source": [
    "# Set up environment and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T23:44:57.515080Z",
     "start_time": "2023-03-10T23:18:51.863147Z"
    },
    "code_folding": [
     33,
     81,
     138
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from infrastructure_230101a import match_rows_in_matrix, normalize_each_row_by_sum, tern2cart, similarity_matrix\n",
    "from network_ai_individual1_230101a import control_ai\n",
    "from instruments_230101a import instrument_measure, instrument_synthesis\n",
    "from global_variables_and_monitors_230101a import sample_pool_manager, performance_monitor, mat_repository, agt_repository\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from uuid import uuid4\n",
    "import timeit\n",
    "\n",
    "from collections import namedtuple\n",
    "import simpy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from scipy.spatial import distance as scipy_dist\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from numpy.random import default_rng\n",
    "\n",
    "class lab:\n",
    "    def __init__(self, env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, spm_name):\n",
    "        spm = sample_pool_manager(env, spm_name)\n",
    "        spm.sample_pool_append(samples_to_start_with)\n",
    "        inst_params['sample_pool_manager'] = spm\n",
    "        agt_params['sample_pool_manager'] = spm\n",
    "        \n",
    "        if lab_type == 'independent':\n",
    "            agt_params['combined_acquisition'] = False\n",
    "            # cr_i = None # central mat repository\n",
    "        elif lab_type == 'coreg':\n",
    "            agt_params['combined_acquisition'] = False\n",
    "            cmr = mat_repository(env, spm) # central material repository\n",
    "            agt_params['central_mat_repo'] = cmr\n",
    "        elif lab_type == 'shared_decision':\n",
    "            agt_params['combined_acquisition'] = True\n",
    "            agt_params['comb_acq_weight'] = .25\n",
    "            cmr = mat_repository(env, spm)\n",
    "            agt_params['central_mat_repo'] = cmr\n",
    "            agt_params['central_agt_repo'] = agt_repository() # set up the agent repo\n",
    "            \n",
    "        Measure_Time = inst_params['Measure_Time']\n",
    "        Synth_Time = inst_params['Synth_Time']\n",
    "        \n",
    "        self.meas_fp = []; self.meas_str = []; self.synth = []; self.agent_fp = []; self.agent_str = [];\n",
    "        for i in range(multiples):\n",
    "            print('i:', str(i))\n",
    "            self.meas_fp.append( instrument_measure(env, 'funcprop', inst_params, unique_ID = uuid4().int) )#[instrument_measure(env, names_[i])) for i in range(4)]\n",
    "            self.meas_str.append( instrument_measure(env, 'structure', inst_params, unique_ID = uuid4().int) )\n",
    "            self.synth.append( instrument_synthesis(env, inst_params, unique_ID = uuid4().int) )\n",
    "            self.agent_fp.append( control_ai(env, 'funcprop', uuid4().int, self.synth[i], self.meas_fp[i], agt_params) )\n",
    "            self.agent_str.append( control_ai(env, 'structure', uuid4().int, self.synth[i], self.meas_str[i], agt_params) )\n",
    "            print('multiple:', len(self.meas_fp), self.meas_fp)\n",
    "            \n",
    "        if lab_type == 'shared_decision':\n",
    "            agent_list = self.agent_fp + self.agent_str\n",
    "            # agent_list.append(self.agent_fp)\n",
    "            # agent_list.append(self.agent_str)\n",
    "            agt_params['central_agt_repo'].init_agt_repo(agent_list) # initialize agent repo\n",
    "            print(agent_list)\n",
    "        \n",
    "        for i in range(multiples):\n",
    "            print('synth', str(self.synth[i].unique_ID)[:5])\n",
    "            print('meas_funcprop', str(self.meas_fp[i].unique_ID)[:5])\n",
    "            print('meas_structure', str(self.meas_str[i].unique_ID)[:5])\n",
    "            print('agent_funcprop', str(self.agent_fp[i].unique_ID)[:5])\n",
    "            print('agent_structure', str(self.agent_str[i].unique_ID)[:5])  \n",
    "\n",
    "def simple_challenge(RUNTIME, save_dir):\n",
    "    \n",
    "    # start with 2 samples in sample pool. 'Composition' is a number sampled between 0 and 2.\n",
    "    num_init_pool = 2\n",
    "    pool_init = np.asarray([2., 17.]) #np.random.uniform(0,2,(num_init_pool)) #np.random.permutation(N)[0:num_init_pool]\n",
    "\n",
    "    # Instantiate the environment.\n",
    "    env = simpy.Environment()\n",
    "\n",
    "    ### ------------ Real Data -----------------------------------------------\n",
    "    Measure_Time = 1\n",
    "    Synth_Time= 1\n",
    "    verbose = False\n",
    "    save_figs = False\n",
    "    Sample = namedtuple('Sample', 'sample_index, composition, structure, funcprop, status')\n",
    "    samples_to_start_with = [Sample(idx, original_idx, None, None, None) for idx, original_idx in enumerate(pool_init)]\n",
    "\n",
    "    # Parameters\n",
    "    meas_type = 'sim_Raman_real'\n",
    "    agt_params = {'AL_PM_method':'entropy', 'AL_BO_method':'UCB', 'x_range':np.asarray([2., 17.1, 0.1]), 'phase_mapping':None, 'combined_acquisition':False, 'comb_acq_weight':0.,\n",
    "                  'sample_pool_manager':None, 'meas_type':meas_type,\n",
    "                  'central_mat_repo':None, 'central_agt_repo':None,\n",
    "                  'use_cp':True, #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                  'verbose':verbose,\n",
    "                  'save_figs':save_figs}\n",
    "    agt_params['phase_mapping'] = {'num_clusters':3, 'stdiv_XRD':.1, 'stdiv_C':1}\n",
    "    inst_params = {'Measure_Time':Measure_Time, 'Synth_Time':Synth_Time, \n",
    "                   'save_dir':None,\n",
    "                   'meas_type':meas_type,\n",
    "                   'sample_pool_manager':None}\n",
    "    inst_params['save_dir']= save_dir\n",
    "    \n",
    "    # # !!!!!!! INDEPENDENT !!!!!!!!!!!!!!!\n",
    "\n",
    "    # each lab has its own pool\n",
    "    multiples = 2\n",
    "\n",
    "    meas_type = 'sim_Raman_real'\n",
    "    inst_params['meas_type'] = meas_type\n",
    "    agt_params['meas_type'] = meas_type\n",
    "    lab_type = 'independent'\n",
    "    labsIR2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsIR2 = []\n",
    "    \n",
    "    # coreg\n",
    "    lab_type = 'coreg'\n",
    "    labsCR2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsCR2 = []\n",
    "    \n",
    "    # # coreg\n",
    "    lab_type = 'shared_decision'\n",
    "    labsDR2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsDR2 = []\n",
    "    env.run(until=RUNTIME)\n",
    "    \n",
    "    return [labsIR2, labsCR2, labsDR2]\n",
    "\n",
    "def hard_challenge(RUNTIME, save_dir):\n",
    "    \n",
    "    # start with 2 samples in sample pool. 'Composition' is a number sampled between 0 and 2.\n",
    "    num_init_pool = 2\n",
    "    pool_init = np.asarray([2., 17.]) #np.random.uniform(0,2,(num_init_pool)) #np.random.permutation(N)[0:num_init_pool]\n",
    "\n",
    "    # Instantiate the environment.\n",
    "    env = simpy.Environment()\n",
    "\n",
    "    ### ------------ Real Data -----------------------------------------------\n",
    "    Measure_Time = 1\n",
    "    Synth_Time= 1\n",
    "    verbose = False\n",
    "    save_figs = False\n",
    "    Sample = namedtuple('Sample', 'sample_index, composition, structure, funcprop, status')\n",
    "    samples_to_start_with = [Sample(idx, original_idx, None, None, None) for idx, original_idx in enumerate(pool_init)]\n",
    "\n",
    "    # Parameters\n",
    "    meas_type = 'sim_Raman_hard'\n",
    "    agt_params = {'AL_PM_method':'entropy', 'AL_BO_method':'UCB', 'x_range':np.asarray([2., 17.1, 0.1]), 'phase_mapping':None, 'combined_acquisition':False, 'comb_acq_weight':0.,\n",
    "                  'sample_pool_manager':None, 'meas_type':meas_type,\n",
    "                  'central_mat_repo':None, 'central_agt_repo':None,\n",
    "                  'use_cp':True, #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                  'verbose':verbose,\n",
    "                  'save_figs':save_figs}\n",
    "    agt_params['phase_mapping'] = {'num_clusters':3, 'stdiv_XRD':.1, 'stdiv_C':1}\n",
    "    inst_params = {'Measure_Time':Measure_Time, 'Synth_Time':Synth_Time, \n",
    "                   'save_dir':None,\n",
    "                   'meas_type':meas_type,\n",
    "                   'sample_pool_manager':None}\n",
    "    # inst_params['save_dir']= r'/content/gdrive/My Drive/Research/jupyter/Networked ML/gpyflow_model'\n",
    "    # inst_params['save_dir']= r'G:/My Drive/Research/jupyter/Networked ML/gpyflow_model'\n",
    "    inst_params['save_dir']= save_dir #r'/home/gkusne/jupyter/NetworkedAgents/gpyflow_model'\n",
    "    \n",
    "    # # !!!!!!! INDEPENDENT !!!!!!!!!!!!!!!\n",
    "\n",
    "    # each lab has its own pool\n",
    "    multiples = 2\n",
    "\n",
    "    # # ### ------------ Hard Data -----------------------------------------------\n",
    "    meas_type = 'sim_Raman_hard'\n",
    "    inst_params['meas_type'] = meas_type\n",
    "    agt_params['meas_type'] = meas_type\n",
    "    lab_type = 'independent'\n",
    "    labsIH2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsIH2 = []\n",
    "    \n",
    "    # coreg\n",
    "    lab_type = 'coreg'\n",
    "    labsCH2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsCH2 = []\n",
    "    \n",
    "    # # coreg\n",
    "    lab_type = 'shared_decision'\n",
    "    labsDH2 = [lab(env, inst_params, agt_params, samples_to_start_with, lab_type, multiples, str(i)) for i in range(1)]\n",
    "    # labsDH2 = []\n",
    "    \n",
    "    env.run(until=RUNTIME)\n",
    "    \n",
    "    return [labsIH2, labsCH2, labsDH2]\n",
    "\n",
    "\n",
    "# save_dir = r'/content/gdrive/My Drive/Research/jupyter/Networked ML/gpyflow_model'\n",
    "save_dir = r'G:/My Drive/Research/jupyter/Networked ML/gpyflow_model'\n",
    "# save_dir = r'/home/gkusne/jupyter/NetworkedAgents/gpyflow_model'\n",
    "    \n",
    "RUNTIME = 200\n",
    "labsR_ = []\n",
    "labsH_ = []\n",
    "for ii in range(1):\n",
    "    [labsIR2a, labsCR2a, labsDR2a] = simple_challenge(RUNTIME, save_dir)\n",
    "    [labsIH2a, labsCH2a, labsDH2a] = hard_challenge(RUNTIME, save_dir)\n",
    "    labsR_.append( [labsIR2a, labsCR2a, labsDR2a] )\n",
    "    labsH_.append( [labsIH2a, labsCH2a, labsDH2a] )\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T22:05:19.984763Z",
     "start_time": "2023-02-14T22:05:18.396153Z"
    },
    "code_folding": [
     37
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting with multiples\n",
    "\n",
    "from global_variables_and_monitors_201008a import sample_pool_manager, performance_monitor, mat_repository, agt_repository\n",
    "from datetime import datetime\n",
    "\n",
    "def simple_regret(val_by_iter, meas_type):\n",
    "    if meas_type == 'sim_Raman_real':\n",
    "        TRUE_MAX = 109.92\n",
    "    elif meas_type == 'sim_Raman_hard':\n",
    "        X = np.round_(np.arange(2.,17.,.1), 2)\n",
    "        yg0 = .2*np.exp(-.5*(X-5.)**2 / 2.)\n",
    "        yg0[X > 8.] = 0\n",
    "        yg1 = .2*np.exp(-.5*(X-11.)**2 / 2.)\n",
    "        yg1[X < 8.] = 0\n",
    "        yg1[X > 14.1] = 0\n",
    "        yg2 = 2*np.exp(-.5*(X-14.1)**2 / 1.)\n",
    "        yg2[X < 14.1] = 0\n",
    "        y = yg0 + yg1 + yg2\n",
    "        y = 24.*y+60.\n",
    "        TRUE_MAX = y.max()\n",
    "    measure_regret = np.zeros(val_by_iter.shape)\n",
    "    max_by_iter = np.maximum.accumulate(val_by_iter)\n",
    "    sr = 1 - max_by_iter/TRUE_MAX\n",
    "    return sr\n",
    "    \n",
    "def plot_history_f(agent, linestr, meas_type):    \n",
    "    sr = simple_regret(agent.perf_monitor, meas_type)\n",
    "    plt.plot(sr * 100, linestr)\n",
    "    #plt.title(agent.name + ' coreg:' + str(agent.use_coreg))\n",
    "    plt.ylabel('%')\n",
    "    return sr\n",
    "\n",
    "def plot_history_s(agent, linestr): \n",
    "    fmi = performance_monitor.phase_mapping_performance(agent.perf_monitor, agt_params['x_range'])\n",
    "    plt.plot(fmi, linestr)\n",
    "    return fmi\n",
    "    #plt.title(agent.name + ' coreg:' + str(agent.use_coreg))\n",
    "\n",
    "def runs2data(labsI, meas_type, agt_params):\n",
    "    multiples = 2\n",
    "    fp_scoreI = []\n",
    "    pm_scoreI = []\n",
    "    for idx, lab_unit in enumerate(labsI):\n",
    "        simple_regret_ = []\n",
    "        fmi_ = []\n",
    "        template = np.empty((30))\n",
    "        template[:] = np.NaN\n",
    "        for item in lab_unit:\n",
    "            for i in range(multiples):\n",
    "                sr = template.copy()\n",
    "                fmi = template.copy()\n",
    "\n",
    "                sr_temp = simple_regret(item.agent_fp[i].perf_monitor, meas_type)\n",
    "                sr[:sr_temp.flatten().shape[0]] = sr_temp.flatten()\n",
    "                simple_regret_.append(sr.flatten()[None,:])\n",
    "\n",
    "                fmi_temp = performance_monitor.phase_mapping_performance(item.agent_str[i].perf_monitor, agt_params['x_range'])\n",
    "                fmi[:fmi_temp.flatten().shape[0]] = fmi_temp.flatten()\n",
    "                fmi_.append(fmi.flatten()[None,:])\n",
    "\n",
    "        simple_regret_ = np.vstack(simple_regret_)\n",
    "        fmi_ = np.vstack(fmi_)\n",
    "        fp_scoreI.append(simple_regret_)\n",
    "        pm_scoreI.append(fmi_)\n",
    "    return fp_scoreI, pm_scoreI\n",
    "\n",
    "meas_type = 'sim_Raman_hard'\n",
    "agt_params = {'AL_PM_method':'risk_min', 'AL_BO_method':'UCB', 'x_range':np.asarray([2., 17.1, 0.1]), 'phase_mapping':None, 'combined_acquisition':False, 'comb_acq_weight':0.,\n",
    "                  'sample_pool_manager':None, 'meas_type':meas_type,\n",
    "                  'central_mat_repo':None, 'central_agt_repo':None,\n",
    "                  'use_cp':True}\n",
    "    \n",
    "meas_type = 'sim_Raman_real'\n",
    "fp_score_R = []; pm_score_R = []\n",
    "for ii in range(len(labsR_)):\n",
    "    fp_score_temp, pm_score_temp = runs2data(labsR_[ii], meas_type, agt_params)\n",
    "    fp_score_R.append(fp_score_temp)\n",
    "    pm_score_R.append(pm_score_temp)\n",
    "    \n",
    "meas_type = 'sim_Raman_hard'\n",
    "fp_score_H = []; pm_score_H = []\n",
    "for ii in range(len(labsH_)):\n",
    "    fp_score_temp, pm_score_temp = runs2data(labsH_[ii], meas_type, agt_params)\n",
    "    fp_score_H.append(fp_score_temp)\n",
    "    pm_score_H.append(pm_score_temp)\n",
    "    \n",
    "data = [fp_score_R, pm_score_R, fp_score_H, pm_score_H]\n",
    "\n",
    "import pickle\n",
    "time_string = datetime.now().strftime(\"%H%M%S\")\n",
    "pickle.dump( data, open( 'results_230223a_' + time_string + '.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T00:52:26.641302Z",
     "start_time": "2023-02-27T00:52:23.403380Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dn = r'G:\\\\My Drive\\\\Research\\\\jupyter\\\\Networked ML\\\\results_wsum_maxof2\\\\'\n",
    "\n",
    "def gen_summary_figures_and_save(dn):\n",
    "    fps_R = []; pms_R = []; fps_H = []; pms_H = [];\n",
    "    \n",
    "    fn_ = [f for f in listdir(dn) if isfile(join(dn, f)) and f[-3:]=='pkl']\n",
    "    \n",
    "    for i in range(len(fn_)):\n",
    "        t = pickle.load(open(dn + fn_[i], \"rb\" ))\n",
    "        fps_R += t[0]\n",
    "        pms_R += t[1]\n",
    "        fps_H += t[2]\n",
    "        pms_H += t[3]\n",
    "\n",
    "    print(len(fps_R))\n",
    "    data = [fps_R, pms_R, fps_H, pms_H]\n",
    "\n",
    "    print(data[0][0][0][0])\n",
    "\n",
    "    fps_R, pms_R, fps_H, pms_H = data\n",
    "    print(len(data),len(data[0]),len(data[0][0]),len(data[0][0][0]),data[0][0][0][0])\n",
    "    # d_ = np.nan * np.ones((len(data),len(data[0]),len(data[0][0]),len(data[0][0][0]),data[0][0][0][0].shape[0]))\n",
    "    d_ = np.zeros((len(data),len(data[0]),len(data[0][0]),len(data[0][0][0]),data[0][0][0][0].shape[0]))\n",
    "    print(d_.shape)\n",
    "    for i in range(len(data[0][0][0])): # multiplicity of agents\n",
    "        for j in range(len(data[0][0])): # agent type\n",
    "            for k in range(len(data[0])): # number of repetitions\n",
    "                for l in range(len(data)): # l is: fp Real, pm Real, fp Hard, pm Hard\n",
    "                    d_[l,k,j,i,:] = data[l][k][j][i]\n",
    "                    \n",
    "    d_[d_ == np.nan] = 0\n",
    "    \n",
    "    print('nans:',np.sum(d_.flatten() == np.nan))\n",
    "    \n",
    "\n",
    "    s_ = np.zeros((4,3,30))\n",
    "    for i in range(d_.shape[0]): # challenge type\n",
    "        for j in range(d_.shape[2]): # agent type\n",
    "            for k in range(d_.shape[4]): # of time steps\n",
    "                # s_: std for challenge type, agent type, and time step.\n",
    "                s_[i,j,k] = 100*np.nanstd( d_[i,:,j,:,k].flatten() ) # std over number of repetitions and multiplicity\n",
    "\n",
    "    ci_ = np.zeros((4,3,30))        \n",
    "    for i in range(d_.shape[0]): # for each challenge type\n",
    "        for j in range(d_.shape[2]): # for each agent type\n",
    "            for k in range(d_.shape[4]): # for each time step\n",
    "                temp = d_[i,:,j,:,k].flatten()\n",
    "                temp = temp[~np.isnan(temp)]\n",
    "                out0 = st.t.interval(alpha=0.95, df=temp.shape[0]-1, loc=np.mean(temp), scale=st.sem(temp)) \n",
    "                # CI for challenge type, agent type, and time step.\n",
    "                ci_[i,j,k] = out0[0]\n",
    "\n",
    "    m_ = d_.copy()\n",
    "    # m_: mean over multiplicity of agents.\n",
    "    m_ = np.nanmean(m_,axis = 3).squeeze()\n",
    "    # m_: mean over repetitions.\n",
    "    m_ = 100*np.nanmean(m_,axis = 1).squeeze()\n",
    "\n",
    "    linestr = 'rbg'\n",
    "    challenge_name = ['fps_R', 'pms_R', 'fps_H', 'pms_H']\n",
    "    agent_type = ['simple','coreg','coreg+']\n",
    "\n",
    "    linestr = 'rbg'\n",
    "    \n",
    "#     plt.figure(figsize=(3,3))\n",
    "#     for i in range(4):\n",
    "#         plt.subplot(2,2,i+1)\n",
    "#         for j in range(3):\n",
    "#             plt.plot(m_[i,j,:],c=linestr[j])\n",
    "#             L = np.isnan(m_[i,j,:]).sum()\n",
    "#             plt.plot(m_[i,j,:] + s_[i,j,:],'--'+linestr[j])\n",
    "#             plt.plot(np.maximum( m_[i,j,:] - s_[i,j,:], np.zeros((30))),'--'+linestr[j])\n",
    "#             plt.fill_between(np.arange(30),np.maximum( m_[i,j,:] - s_[i,j,:], np.zeros((30))),\n",
    "#                              m_[i,j,:] + s_[i,j,:],\n",
    "#                              alpha=0.1, color = linestr[j])\n",
    "#             plt.title('mean+std,' + challenge_name[i] )\n",
    "#     plt.title('STD')    \n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.figure(figsize=(2,2))\n",
    "        # plt.subplot(2,2,i+1)\n",
    "        for j in range(3):\n",
    "            plt.plot(m_[i,j,:],c=linestr[j])\n",
    "            L = np.isnan(m_[i,j,:]).sum()\n",
    "            plt.plot(m_[i,j,:] + ci_[i,j,:],'--'+linestr[j])\n",
    "            plt.plot(np.maximum( m_[i,j,:] - ci_[i,j,:], np.zeros((30))),'--'+linestr[j])\n",
    "            plt.fill_between(np.arange(30),np.maximum( m_[i,j,:] - ci_[i,j,:], np.zeros((30))),\n",
    "                             m_[i,j,:] + ci_[i,j,:],\n",
    "                             alpha=0.1, color = linestr[j])\n",
    "        plt.title('mean+CI 95%,' + challenge_name[i] )\n",
    "        plt.xticks(np.arange(0,11,2))\n",
    "        plt.savefig(dn + challenge_name[i] + 'summary.svg', format='svg')\n",
    "        \n",
    "    # plt.title('CI 95%')\n",
    "    \n",
    "    \n",
    "\n",
    "    for k in range(8):\n",
    "        for j in range(2):\n",
    "            plt.figure(figsize=(2,2))\n",
    "            for l in range(3):\n",
    "                # plt.subplot(1,2,j+1)\n",
    "                plt.plot(100*d_[j*2,k,l,:,:].T, color = linestr[l])\n",
    "                plt.title(challenge_name[j*2])\n",
    "            plt.xticks(np.arange(0,11,2))\n",
    "            plt.savefig(dn + challenge_name[j*2] + str(k) + 'summary.svg', format='svg')\n",
    "            \n",
    "    # plt.figure()\n",
    "    # plt.plot([0,1],[0,1])\n",
    "    # plt.show()\n",
    "    \n",
    "\n",
    "dn = r'G:\\\\My Drive\\\\Research\\\\jupyter\\\\Networked ML\\\\results_230223a_wsum_max2_500samples\\\\'\n",
    "gen_summary_figures_and_save(dn)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zoc3ZgH0vzsd",
    "lWa8uv7pdkRr",
    "_KXu90OQadIr",
    "2mcZGK0Yajxx",
    "gMfzyJWuyhtm",
    "_7LmBsIwzJCm",
    "Pl9yYQbG5K-P",
    "qCvVbciD5iFS"
   ],
   "name": "Colab - Networked Autonomous Science - 220617a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "287px",
    "width": "589px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 422.383334,
   "position": {
    "height": "40px",
    "left": "481.225px",
    "right": "20px",
    "top": "125px",
    "width": "604.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
